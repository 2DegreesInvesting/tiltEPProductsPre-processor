{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Import the required libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import dedupe\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mandatory input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_catalogue= \"../../data/example_data/input/scraped_data/scraped_EP_products_catalogue.csv\"\n",
    "dedup_settings_file = '../../dedupe_files/dedup_learned_settings'\n",
    "dedup_training_file = '../../dedupe_files/dedup_training.json'\n",
    "rl_settings_file = '../../dedupe_files/record_linkage_learned_settings'\n",
    "rl_training_file = '../../dedupe_files/record_linkage_training.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_file_location =\"../../data/example_data/output/base_data/base_translated_products.csv\"\n",
    "base_data_deduped_file =  \"../../data/example_data/output/base_data/base_deduped.csv\"\n",
    "base_data_output_file = '../../data/example_data/output/base_data/base_linked_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new Italy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "italy_data_file_location = \"../../data/example_data/output/italy_data/italy_translated_products.csv\"\n",
    "italy_deduped_file = \"../../data/example_data/output/italy_data/italy_deduped.csv\"\n",
    "italy_output_file = '../../data/example_data/output/italy_data/italy_linked_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(column):\n",
    "    column = unidecode(column)\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    column = re.sub('-', '', column)\n",
    "    column = re.sub('/', ' ', column)\n",
    "    column = re.sub(\"'\", '', column)\n",
    "    column = re.sub(\",\", '', column)\n",
    "    column = re.sub(\":\", ' ', column)\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    if not column:\n",
    "        column = None\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pandas_to_dict(dataframe, which = \"None\", type = \"dedup\"):\n",
    "    data_d = {}\n",
    "    for i, row in dataframe.iterrows():\n",
    "        x = zip(row.index, row.values)\n",
    "        clean_row = dict([(k, preProcess(str(v))) for (k, v) in x])\n",
    "        if type != \"dedup\":\n",
    "            data_d[which + str(i)] = clean_row\n",
    "        else:\n",
    "            data_d[i] = dict(clean_row)\n",
    "    return data_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seconds_conversion(seconds):\n",
    "    # Convert the time difference to a timedelta object\n",
    "    time_delta = datetime.timedelta(seconds=seconds)\n",
    "\n",
    "    # Extract the hours, minutes, and seconds from the timedelta object\n",
    "    hours = time_delta.seconds // 3600\n",
    "    minutes = (time_delta.seconds % 3600) // 60\n",
    "    seconds = time_delta.seconds % 60\n",
    "    return (hours, minutes, seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dedupe modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deduplication module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplication(file, settings, training, write = False, out = \"None\"):\n",
    "   \"\"\"\n",
    "   This function deduplicates the dataframe using the dedupe library.\n",
    "\n",
    "   Args:\n",
    "       file (str or pd.Dataframe): The path to the file to be deduplicated or a pandas dataframe.\n",
    "       settings (str): The path to the settings file.\n",
    "       training (str): The path to the training file.\n",
    "       write (bool): Indicates whether to write the deduplicated output to file.\n",
    "       out (str): The path to the output file.\n",
    "    Returns:\n",
    "        df (pd.DataFrame): The deduplicated dataframe.\n",
    "   \"\"\"\n",
    "   # Read the csv files\n",
    "   print('Importing data ...')\n",
    "   if isinstance(file, str):\n",
    "       df = pd.read_csv(file)\n",
    "   else:\n",
    "       df = file\n",
    "\n",
    "   # stage 1: Deduplication using dedupe library\n",
    "   print(\"----Start of stage 1----\")\n",
    "   print('Preparing dedupe data ...')\n",
    "   dedup_data = convert_pandas_to_dict(df, \"dedup\")\n",
    "   if os.path.exists(settings):\n",
    "      print('Settings file found! Reading settings from \"{}\"'.format(settings))\n",
    "      with open(settings, 'rb') as sf:\n",
    "         deduper = dedupe.StaticDedupe(sf)\n",
    "   # If no settings file exists, create train a new linker object\n",
    "   else:\n",
    "      # Define the fields that will be used for the record linkage\n",
    "      fields = [\n",
    "               {'field': 'products_and_services', 'type': 'String'}] # consider Text type instead of String\n",
    "      \n",
    "      # Create a new linker object and pass the fields to it\n",
    "      deduper = dedupe.Dedupe(fields)\n",
    "      print(\"Preparing training...\")\n",
    "      if os.path.exists(training):\n",
    "         print('Reading labeled examples from ', training)\n",
    "         with open(training) as tf:\n",
    "               deduper.prepare_training(dedup_data,\n",
    "                                       training_file=tf)\n",
    "      else:\n",
    "         # Prepare the linker object for training using the two datasets\n",
    "         deduper.prepare_training(dedup_data)\n",
    "      # Start the active labeling\n",
    "      print('Starting active labeling...')\n",
    "      dedupe.console_label(deduper)\n",
    "      # Train the linker object using the active labeling as additional input\n",
    "      print(\"Training...\")\n",
    "      deduper.train()\n",
    "      print(\"Training finished!\")\n",
    "      # write the labelled training examples to disk\n",
    "      with open(training, 'w') as tf:\n",
    "         deduper.write_training(tf)\n",
    "      # write the settings file to disk\n",
    "      with open(settings, 'wb') as sf:\n",
    "         deduper.write_settings(sf)\n",
    "\n",
    "   print('Clustering..')\n",
    "   clustered_dupes = deduper.partition(dedup_data, 0.5)\n",
    "   print('Clustering finished!. {} duplicates found'.format(len(df)-len(clustered_dupes)))\n",
    "\n",
    "   print('Dropping duplicates...')\n",
    "   rows_to_drop = []\n",
    "   for _, (records, scores) in enumerate(clustered_dupes):\n",
    "      rows_to_drop.append(records[1:])\n",
    "\n",
    "   # flatten list of lists\n",
    "   rows_to_drop = [item for sublist in rows_to_drop for item in sublist]\n",
    "   df = df.drop(df.index[rows_to_drop])\n",
    "   \n",
    "   print (\"Duplicates dropped!\")\n",
    "   print(\"----Finished stage 1----\")\n",
    "   # reorder the columns\n",
    "   df = df[['products_id', 'manual_processed_products_and_services', 'automatic_processed_products_and_services']]\n",
    "\n",
    "   if write: \n",
    "      # rename product_and_services column to automated_processed_products_and_services\n",
    "      df = df.rename(columns={'products_and_services': 'automatic_processed_products_and_services'}).drop(columns=['Unnamed: 0'])\n",
    "      # if write_path string contains the word base then we are also merging it with the manual clustered data\n",
    "      if 'base' in out:\n",
    "         # read in manual clustered datax \n",
    "         manual_clustered_df = pd.read_csv('../../data/example_data/input/manual_clustering.csv')\n",
    "         # merge the two dataframes on products_id\n",
    "         df = pd.merge(manual_clustered_df, df, on='products_id')\n",
    "         # drop cluster_id, clustered_delimited_id,delimited_id\n",
    "         df = df.drop(['clustered_id', 'clustered_delimited_id', 'delimited_id', 'delimited_products_id'], axis=1).rename(columns = {\"clustered\": \"manual_processed_products_and_services\"})\n",
    "         # reorder columns\n",
    "         df = df[['products_id','raw_products_and_services', 'manual_processed_products_and_services', 'automatic_processed_products_and_services']]\n",
    "      print('Writing deduplicated output to file...')\n",
    "      df.to_csv(out, index=False)\n",
    "      \n",
    "   return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record Linkage module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def record_linkage(left_df, right_df, settings, training, write = False, out = \"None\"):\n",
    "#     \"\"\"\n",
    "#     This function performs record linkage on the two dataframes using the dedupe library.\n",
    "\n",
    "#     Args:\n",
    "#         left_df (pd.DataFrame): The left dataframe.\n",
    "#         right_df (pd.DataFrame): The right dataframe.\n",
    "#         settings (str): The path to the settings file.\n",
    "#         training (str): The path to the training file.\n",
    "#         write (bool): Indicates whether to write the deduplicated output to file.\n",
    "#         out (str): The path to the output file.\n",
    "#     Returns:\n",
    "#         merged_df (pd.DataFrame): The merged dataframe.\n",
    "#     \"\"\"\n",
    "#     print('Importing data ...')\n",
    "#     if isinstance(left_df, str):\n",
    "#         root_l_df = pd.read_csv(left_df)\n",
    "#         root_r_df = pd.read_csv(right_df)\n",
    "#     else:\n",
    "#         root_l_df = left_df.copy()\n",
    "#         root_r_df = right_df.copy()\n",
    "\n",
    "#     # Stage 1: Direct products_and_services linkage using merging\n",
    "#     print(\"----Start of stage 1----\")\n",
    "#     print('Directly merging data...')\n",
    "#     # Merge the two dataframes based on the 'products_and_services' column\n",
    "#     merged_df = root_l_df.merge(root_r_df, on='products_and_services', how='left', suffixes=['_x', '_y']).drop(columns=\"ID\")\n",
    "#     merged_df = merged_df.merge(root_r_df, left_on='products_id_y', right_on='products_id', how=\"left\").drop(columns=[\"ID\",\"products_id\"])\n",
    "#     # Create a new dataframe that contains rows from company_based_p_and_s that could not be directly matched\n",
    "#     non_matched_products = merged_df[merged_df.isna().any(axis=1)].drop(columns=[\"products_id_y\", \"products_and_services_y\"]).rename(columns={\"products_and_services_x\": \"products_and_services\"})\n",
    "#     # Get the percentage of products_and_services that could be directly matched\n",
    "#     percentage_matched = len(merged_df.dropna())/len(root_l_df)*100\n",
    "#     print('Percentage of products_and_services that could be directly matched: {0:.2f}%'.format(percentage_matched))\n",
    "#     print(\"----Finished stage 1----\\n\")\n",
    "\n",
    "#     # Stage 2: Remaining products_and_services linkage using dedupe\n",
    "#     print(\"----Start of stage 2----\")\n",
    "#     print('Preparing record linkage data...')\n",
    "#     # Convert the dataframes to dictionaries\n",
    "#     linkage_data_1 = convert_pandas_to_dict(non_matched_products, \"left\", \"linkage\")\n",
    "#     linkage_data_2 = convert_pandas_to_dict(root_r_df, \"right\", \"linkage\")\n",
    "#     print('Attempting products_and_services linkage on the remainder using dedupe...')\n",
    "#     # Check if a settings file already exists and use if can be found\n",
    "#     if os.path.exists(settings):\n",
    "#         print('Settings file found! Reading settings from \"{}\"'.format(settings))\n",
    "#         with open(settings, 'rb') as sf:\n",
    "#             linker = dedupe.StaticRecordLink(sf)\n",
    "#     # If no settings file exists, create train a new linker object\n",
    "#     else:\n",
    "#         # Define the fields that will be used for the record linkage\n",
    "#         fields = [\n",
    "#                 {'field': 'products_and_services', 'type': 'String'}] # consider Text type instead of String\n",
    "        \n",
    "#         # Create a new linker object and pass the fields to it\n",
    "#         linker = dedupe.RecordLink(fields)\n",
    "#         print(\"Preparing training...\")\n",
    "#         if os.path.exists(training):\n",
    "#             print('Reading labeled examples from ', training)\n",
    "#             with open(training) as tf:\n",
    "#                 linker.prepare_training(linkage_data_1,\n",
    "#                                         linkage_data_2,\n",
    "#                                         training_file=tf,\n",
    "#                                         sample_size=10000)\n",
    "#         else:\n",
    "#             # Prepare the linker object for training using the two datasets\n",
    "#             linker.prepare_training(linkage_data_1, linkage_data_2, sample_size=10000)\n",
    "#         # Start the active labeling\n",
    "#         print('Starting active labeling...')\n",
    "#         dedupe.console_label(linker)\n",
    "#         # Train the linker object using the active labeling as additional input\n",
    "#         print(\"Training...\")\n",
    "#         linker.train()\n",
    "#         print(\"Training finished!\")\n",
    "#         # write the labelled training examples to disk\n",
    "#         with open(training, 'w') as tf:\n",
    "#             linker.write_training(tf)\n",
    "#         # write the settings file to disk\n",
    "#         with open(settings, 'wb') as sf:\n",
    "#             linker.write_settings(sf)\n",
    "#     # Perform the record linkage\n",
    "#     print('Performing linking...')\n",
    "#     linked_records = linker.join(linkage_data_1, linkage_data_2, 0.0)\n",
    "#     print('Succesfully linked {} records'.format(len(linked_records)))\n",
    "#     for _, (cluster, score) in enumerate(linked_records):\n",
    "#         non_matched_products.loc[int(re.search(r\"\\d+\", cluster[0]).group()), 'products_and_services_y'] = root_r_df.loc[int(re.search(r\"\\d+\", cluster[1]).group()), 'products_and_services']\n",
    "#         non_matched_products.loc[int(re.search(r\"\\d+\", cluster[0]).group()), 'products_id_y'] = root_r_df.loc[int(re.search(r\"\\d+\", cluster[1]).group()), 'products_id']\n",
    "    \n",
    "#     merged_df = merged_df.fillna(non_matched_products)\n",
    "#     merged_df = merged_df.rename(columns = {\"products_id_x\": \"products_id\", \n",
    "#                                                                          \"products_and_services_x\": \"automatic_processed_products_and_services\",\n",
    "#                                                                          \"products_id_y\": \"linked_EP_products_id\",\n",
    "#                                                                          \"products_and_services_y\": \"linked_EP_products_and_services\"})\n",
    "#     print(\"Coverage increased to {0:.2f}%\".format(len(merged_df.dropna())/len(root_l_df)*100))\n",
    "#     print(\"----Finished stage 2----\\n\")\n",
    "#     if write:\n",
    "#         print('Writing results to \"{}\"'.format(out))\n",
    "#         merged_df.to_csv(out, index=False)\n",
    "\n",
    "#     return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dedup_and_link(df, ep_df_path, out, dedup_settings_file, dedup_training_file, linking_settings_file, linking_training_file):\n",
    "#     # Start timer\n",
    "#     print(\"\\n/=========== Dedup x Record Linkage started ===========/\")\n",
    "#     start_time = time.time()\n",
    "#     # Phase 1: applying deduplication module to the data\n",
    "#     print(\"/=========== Start of phase 1: Deduplication ===========/\")\n",
    "#     deduped_data = deduplication(df, dedup_settings_file, dedup_training_file)\n",
    "\n",
    "#     # Phase 2: applying record linkage module to the data\n",
    "#     print(\"\\n\\n/=========== Start of phase 2: Record Linkage ===========/\")\n",
    "#     linked_data = record_linkage(deduped_data, pd.read_csv(ep_df_path), linking_settings_file, linking_training_file)\n",
    "#     end_time = time.time()\n",
    "\n",
    "#     print(\"\\n/=========== Dedup x Record Linkage finished. Duration: {} hours, {} minutes, {} seconds ===========/\".format(*seconds_conversion(end_time - start_time)))\n",
    "\n",
    "#     print('Writing results to \"{}\"'.format(out))\n",
    "#     linked_data.to_csv(out, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.1 Duplicate removal and Europages linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dedup_and_link(base_data_file_location, ep_catalogue, base_data_output_file, dedup_settings_file, dedup_training_file, rl_settings_file, rl_training_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data ...\n",
      "----Start of stage 1----\n",
      "Preparing dedupe data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdeduplication\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_data_file_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdedup_settings_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdedup_training_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbase_data_deduped_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 45\u001b[0m, in \u001b[0;36mdeduplication\u001b[1;34m(file, settings, training, write, out)\u001b[0m\n\u001b[0;32m     41\u001b[0m          deduper\u001b[38;5;241m.\u001b[39mprepare_training(dedup_data,\n\u001b[0;32m     42\u001b[0m                                  training_file\u001b[38;5;241m=\u001b[39mtf)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m    \u001b[38;5;66;03m# Prepare the linker object for training using the two datasets\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m    \u001b[43mdeduper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdedup_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Start the active labeling\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting active labeling...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\selsabrouty\\AppData\\Local\\anaconda3\\envs\\new-tilt-nlp-preprocessing\\lib\\site-packages\\dedupe\\api.py:1424\u001b[0m, in \u001b[0;36mDedupe.prepare_training\u001b[1;34m(self, data, training_file, sample_size, blocked_proportion)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;66;03m# We need the active learner to know about all our\u001b[39;00m\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;66;03m# existing training data, so add them to data dictionary\u001b[39;00m\n\u001b[0;32m   1422\u001b[0m examples, y \u001b[38;5;241m=\u001b[39m flatten_training(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_pairs)\n\u001b[1;32m-> 1424\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_learner \u001b[38;5;241m=\u001b[39m \u001b[43mlabeler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDedupeDisagreementLearner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1425\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredicates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1426\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_include\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1429\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1431\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_learner\u001b[38;5;241m.\u001b[39mmark(examples, y)\n",
      "File \u001b[1;32mc:\\Users\\selsabrouty\\AppData\\Local\\anaconda3\\envs\\new-tilt-nlp-preprocessing\\lib\\site-packages\\dedupe\\labeler.py:422\u001b[0m, in \u001b[0;36mDedupeDisagreementLearner.__init__\u001b[1;34m(self, candidate_predicates, featurizer, data, index_include)\u001b[0m\n\u001b[0;32m    419\u001b[0m index_include \u001b[38;5;241m=\u001b[39m index_include\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    420\u001b[0m index_include\u001b[38;5;241m.\u001b[39mappend(exact_match)\n\u001b[1;32m--> 422\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocker \u001b[38;5;241m=\u001b[39m \u001b[43mDedupeBlockLearner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate_predicates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_include\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocker\u001b[38;5;241m.\u001b[39mcandidates\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatcher \u001b[38;5;241m=\u001b[39m MatchLearner(featurizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcandidates)\n",
      "File \u001b[1;32mc:\\Users\\selsabrouty\\AppData\\Local\\anaconda3\\envs\\new-tilt-nlp-preprocessing\\lib\\site-packages\\dedupe\\labeler.py:236\u001b[0m, in \u001b[0;36mDedupeBlockLearner.__init__\u001b[1;34m(self, candidate_predicates, data, index_include)\u001b[0m\n\u001b[0;32m    233\u001b[0m sampled_records \u001b[38;5;241m=\u001b[39m sample_records(index_data, N_SAMPLED_RECORDS)\n\u001b[0;32m    235\u001b[0m preds \u001b[38;5;241m=\u001b[39m _filter_canopy_predicates(candidate_predicates, canopies\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_learner \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDedupeBlockLearner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampled_records\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_data\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(sampled_records, N_SAMPLED_RECORD_PAIRS)\n\u001b[0;32m    241\u001b[0m examples_to_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcandidates\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\selsabrouty\\AppData\\Local\\anaconda3\\envs\\new-tilt-nlp-preprocessing\\lib\\site-packages\\dedupe\\training.py:199\u001b[0m, in \u001b[0;36mDedupeBlockLearner.__init__\u001b[1;34m(self, predicates, sampled_records, data)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocker \u001b[38;5;241m=\u001b[39m blocking\u001b[38;5;241m.\u001b[39mFingerprinter(predicates)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocker\u001b[38;5;241m.\u001b[39mindex_all(data)\n\u001b[1;32m--> 199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomparison_cover \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoveredPairs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampled_records\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\selsabrouty\\AppData\\Local\\anaconda3\\envs\\new-tilt-nlp-preprocessing\\lib\\site-packages\\dedupe\\training.py:225\u001b[0m, in \u001b[0;36mDedupeBlockLearner.coveredPairs\u001b[1;34m(blocker, records)\u001b[0m\n\u001b[0;32m    222\u001b[0m pred_cover \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\u001b[38;5;28mset\u001b[39m)\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, record \u001b[38;5;129;01min\u001b[39;00m records\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 225\u001b[0m     blocks \u001b[38;5;241m=\u001b[39m \u001b[43mpredicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m blocks:\n\u001b[0;32m    227\u001b[0m         pred_cover[block]\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\selsabrouty\\AppData\\Local\\anaconda3\\envs\\new-tilt-nlp-preprocessing\\lib\\site-packages\\dedupe\\predicates.py:211\u001b[0m, in \u001b[0;36mCanopyPredicate.__call__\u001b[1;34m(self, record, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     block_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcanopy[doc_id]\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 211\u001b[0m     canopy_members \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m member \u001b[38;5;129;01min\u001b[39;00m canopy_members:\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m member \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcanopy:\n",
      "File \u001b[1;32mc:\\Users\\selsabrouty\\AppData\\Local\\anaconda3\\envs\\new-tilt-nlp-preprocessing\\lib\\site-packages\\dedupe\\levenshtein.py:32\u001b[0m, in \u001b[0;36mLevenshteinIndex.search\u001b[1;34m(self, doc, threshold)\u001b[0m\n\u001b[0;32m     30\u001b[0m matching_docs \u001b[38;5;241m=\u001b[39m Levenshtein_search\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_key, doc, threshold)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m matching_docs:\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_doc_to_id[match] \u001b[38;5;28;01mfor\u001b[39;00m match, _, _ \u001b[38;5;129;01min\u001b[39;00m matching_docs]\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "File \u001b[1;32mc:\\Users\\selsabrouty\\AppData\\Local\\anaconda3\\envs\\new-tilt-nlp-preprocessing\\lib\\site-packages\\dedupe\\levenshtein.py:32\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     30\u001b[0m matching_docs \u001b[38;5;241m=\u001b[39m Levenshtein_search\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_key, doc, threshold)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m matching_docs:\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_doc_to_id[match] \u001b[38;5;28;01mfor\u001b[39;00m match, _, _ \u001b[38;5;129;01min\u001b[39;00m matching_docs]\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "deduplication(base_data_file_location, dedup_settings_file, dedup_training_file, write = True, out = base_data_deduped_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new Italy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dedup_and_link(italy_data_file_location, ep_catalogue, italy_output_file, dedup_settings_file, dedup_training_file, rl_settings_file, rl_training_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data ...\n",
      "----Start of stage 1----\n",
      "Preparing dedupe data ...\n",
      "Settings file found! Reading settings from \"../../dedupe_files/dedup_learned_settings\"\n",
      "Clustering..\n",
      "Clustering finished!. 4 duplicates found\n",
      "Dropping duplicates...\n",
      "Duplicates dropped!\n",
      "----Finished stage 1----\n",
      "Writing deduplicated output to file...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_products_and_services</th>\n",
       "      <th>products_id</th>\n",
       "      <th>automatic_processed_products_and_services</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>persian blue salt</td>\n",
       "      <td>50abde66-58b7-4fb3-a007-1077fa41a010</td>\n",
       "      <td>Persian blue salt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>organic saffron bio</td>\n",
       "      <td>0ea739bc-c3fb-4476-b8d6-96ba5085aa00</td>\n",
       "      <td>organic saffron bio.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>property sale and purchase</td>\n",
       "      <td>126e3677-bb68-481d-96e0-ae0fd8a68db3</td>\n",
       "      <td>Property sale and purchase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tomato processing machines</td>\n",
       "      <td>46b2f19e-f238-4b99-be02-d0fbe46d1e4c</td>\n",
       "      <td>Tomato processing machines.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>christmas gift packages</td>\n",
       "      <td>6ced5115-29ba-4477-87e9-93699dce44c9</td>\n",
       "      <td>Christmas gift packages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3430</th>\n",
       "      <td>stampa offset e digitale</td>\n",
       "      <td>f6ebf929-0e99-4ff0-8f83-e46a30520a47</td>\n",
       "      <td>offset and digital printing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3431</th>\n",
       "      <td>pile turner</td>\n",
       "      <td>6762bafd-8627-4894-a113-686adbc2602a</td>\n",
       "      <td>pile turner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3432</th>\n",
       "      <td>synthetic fibre yarns</td>\n",
       "      <td>856223ec-54f2-44ec-ab37-3c438164fa6b</td>\n",
       "      <td>synthetic fibre yarns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3433</th>\n",
       "      <td>polyester microfibre</td>\n",
       "      <td>2d10565b-21f6-4687-b49c-cf06e0fedabd</td>\n",
       "      <td>polyester microfibre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3434</th>\n",
       "      <td>semi-mat polyester</td>\n",
       "      <td>94cc399d-e957-4173-b192-185c7b7c82f5</td>\n",
       "      <td>semi-mat polyester</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3431 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       raw_products_and_services                           products_id  \\\n",
       "0              persian blue salt  50abde66-58b7-4fb3-a007-1077fa41a010   \n",
       "1            organic saffron bio  0ea739bc-c3fb-4476-b8d6-96ba5085aa00   \n",
       "2     property sale and purchase  126e3677-bb68-481d-96e0-ae0fd8a68db3   \n",
       "3     tomato processing machines  46b2f19e-f238-4b99-be02-d0fbe46d1e4c   \n",
       "4        christmas gift packages  6ced5115-29ba-4477-87e9-93699dce44c9   \n",
       "...                          ...                                   ...   \n",
       "3430    stampa offset e digitale  f6ebf929-0e99-4ff0-8f83-e46a30520a47   \n",
       "3431                 pile turner  6762bafd-8627-4894-a113-686adbc2602a   \n",
       "3432       synthetic fibre yarns  856223ec-54f2-44ec-ab37-3c438164fa6b   \n",
       "3433        polyester microfibre  2d10565b-21f6-4687-b49c-cf06e0fedabd   \n",
       "3434          semi-mat polyester  94cc399d-e957-4173-b192-185c7b7c82f5   \n",
       "\n",
       "     automatic_processed_products_and_services  \n",
       "0                           Persian blue salt.  \n",
       "1                         organic saffron bio.  \n",
       "2                   Property sale and purchase  \n",
       "3                  Tomato processing machines.  \n",
       "4                      Christmas gift packages  \n",
       "...                                        ...  \n",
       "3430               offset and digital printing  \n",
       "3431                               pile turner  \n",
       "3432                     synthetic fibre yarns  \n",
       "3433                      polyester microfibre  \n",
       "3434                        semi-mat polyester  \n",
       "\n",
       "[3431 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deduplication(italy_data_file_location, dedup_settings_file, dedup_training_file, write = True, out = italy_deduped_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
