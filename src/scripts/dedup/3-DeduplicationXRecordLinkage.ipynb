{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Import the required libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import dedupe\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mandatory input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_catalogue= \"../../data/example_data/input/scraped_data/scraped_EP_products_catalogue.csv\"\n",
    "dedup_settings_file = '../../dedupe_files/dedup_learned_settings'\n",
    "dedup_training_file = '../../dedupe_files/dedup_training.json'\n",
    "rl_settings_file = '../../dedupe_files/record_linkage_learned_settings'\n",
    "rl_training_file = '../../dedupe_files/record_linkage_training.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_file_location =\"../../data/example_data/output/base_data/base_translated_products.csv\"\n",
    "base_data_deduped_file =  \"../../data/example_data/output/base_data/base_deduped.csv\"\n",
    "base_data_output_file = '../../data/example_data/output/base_data/base_linked_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new Italy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "italy_data_file_location = \"../../data/example_data/output/italy_data/italy_translated_products.csv\"\n",
    "italy_deduped_file = \"../../data/example_data/output/italy_data/italy_deduped.csv\"\n",
    "italy_output_file = '../../data/example_data/output/italy_data/italy_linked_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(column):\n",
    "    column = unidecode(column)\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    column = re.sub('-', '', column)\n",
    "    column = re.sub('/', ' ', column)\n",
    "    column = re.sub(\"'\", '', column)\n",
    "    column = re.sub(\",\", '', column)\n",
    "    column = re.sub(\":\", ' ', column)\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    if not column:\n",
    "        column = None\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pandas_to_dict(dataframe, which = \"None\", type = \"dedup\"):\n",
    "    data_d = {}\n",
    "    for i, row in dataframe.iterrows():\n",
    "        x = zip(row.index, row.values)\n",
    "        clean_row = dict([(k, preProcess(str(v))) for (k, v) in x])\n",
    "        if type != \"dedup\":\n",
    "            data_d[which + str(i)] = clean_row\n",
    "        else:\n",
    "            data_d[i] = dict(clean_row)\n",
    "    return data_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seconds_conversion(seconds):\n",
    "    # Convert the time difference to a timedelta object\n",
    "    time_delta = datetime.timedelta(seconds=seconds)\n",
    "\n",
    "    # Extract the hours, minutes, and seconds from the timedelta object\n",
    "    hours = time_delta.seconds // 3600\n",
    "    minutes = (time_delta.seconds % 3600) // 60\n",
    "    seconds = time_delta.seconds % 60\n",
    "    return (hours, minutes, seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dedupe modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deduplication module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplication(file, settings, training, write = False, out = \"None\"):\n",
    "   \"\"\"\n",
    "   This function deduplicates the dataframe using the dedupe library.\n",
    "\n",
    "   Args:\n",
    "       file (str or pd.Dataframe): The path to the file to be deduplicated or a pandas dataframe.\n",
    "       settings (str): The path to the settings file.\n",
    "       training (str): The path to the training file.\n",
    "       write (bool): Indicates whether to write the deduplicated output to file.\n",
    "       out (str): The path to the output file.\n",
    "    Returns:\n",
    "        df (pd.DataFrame): The deduplicated dataframe.\n",
    "   \"\"\"\n",
    "   # Read the csv files\n",
    "   print('Importing data ...')\n",
    "   if isinstance(file, str):\n",
    "       df = pd.read_csv(file)\n",
    "   else:\n",
    "       df = file\n",
    "\n",
    "   # stage 1: Deduplication using dedupe library\n",
    "   print(\"----Start of stage 1----\")\n",
    "   print('Preparing dedupe data ...')\n",
    "   dedup_data = convert_pandas_to_dict(df, \"dedup\")\n",
    "   if os.path.exists(settings):\n",
    "      print('Settings file found! Reading settings from \"{}\"'.format(settings))\n",
    "      with open(settings, 'rb') as sf:\n",
    "         deduper = dedupe.StaticDedupe(sf)\n",
    "   # If no settings file exists, create train a new linker object\n",
    "   else:\n",
    "      # Define the fields that will be used for the record linkage\n",
    "      fields = [\n",
    "               {'field': 'products_and_services', 'type': 'String'}] # consider Text type instead of String\n",
    "      \n",
    "      # Create a new linker object and pass the fields to it\n",
    "      deduper = dedupe.Dedupe(fields)\n",
    "      print(\"Preparing training...\")\n",
    "      if os.path.exists(training):\n",
    "         print('Reading labeled examples from ', training)\n",
    "         with open(training) as tf:\n",
    "               deduper.prepare_training(dedup_data,\n",
    "                                       training_file=tf)\n",
    "      else:\n",
    "         # Prepare the linker object for training using the two datasets\n",
    "         deduper.prepare_training(dedup_data)\n",
    "      # Start the active labeling\n",
    "      print('Starting active labeling...')\n",
    "      dedupe.console_label(deduper)\n",
    "      # Train the linker object using the active labeling as additional input\n",
    "      print(\"Training...\")\n",
    "      deduper.train()\n",
    "      print(\"Training finished!\")\n",
    "      # write the labelled training examples to disk\n",
    "      with open(training, 'w') as tf:\n",
    "         deduper.write_training(tf)\n",
    "      # write the settings file to disk\n",
    "      with open(settings, 'wb') as sf:\n",
    "         deduper.write_settings(sf)\n",
    "\n",
    "   print('Clustering..')\n",
    "   clustered_dupes = deduper.partition(dedup_data, 0.5)\n",
    "   print('Clustering finished!. {} duplicates found'.format(len(df)-len(clustered_dupes)))\n",
    "\n",
    "   print('Dropping duplicates...')\n",
    "   rows_to_drop = []\n",
    "   for _, (records, scores) in enumerate(clustered_dupes):\n",
    "      rows_to_drop.append(records[1:])\n",
    "\n",
    "   # flatten list of lists\n",
    "   rows_to_drop = [item for sublist in rows_to_drop for item in sublist]\n",
    "   df = df.drop(df.index[rows_to_drop])\n",
    "   \n",
    "   print (\"Duplicates dropped!\")\n",
    "   print(\"----Finished stage 1----\")\n",
    "   \n",
    "   if write: \n",
    "      # rename product_and_services column to automated_processed_products_and_services\n",
    "      df = df.rename(columns={'products_and_services': 'automatic_processed_products_and_services'}).drop(columns=['Unnamed: 0'])\n",
    "      # if write_path string contains the word base then we are also merging it with the manual clustered data\n",
    "      if 'base' in out:\n",
    "         # read in manual clustered datax \n",
    "         manual_clustered_df = pd.read_csv('../../data/example_data/input/manual_clustering.csv')\n",
    "         # merge the two dataframes on products_id\n",
    "         df = pd.merge(manual_clustered_df, df, on='products_id')\n",
    "         # drop cluster_id, clustered_delimited_id,delimited_id\n",
    "         df = df.drop(['clustered_id', 'clustered_delimited_id', 'delimited_id', 'delimited_products_id'], axis=1).rename(columns = {\"clustered\": \"manual_processed_products_and_services\"})\n",
    "         # reorder columns\n",
    "         df = df[['products_id','raw_products_and_services', 'manual_processed_products_and_services', 'automatic_processed_products_and_services']]\n",
    "      print('Writing deduplicated output to file...')\n",
    "      df.to_csv(out, index=False)\n",
    "      \n",
    "\n",
    "   return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record Linkage module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def record_linkage(left_df, right_df, settings, training, write = False, out = \"None\"):\n",
    "#     \"\"\"\n",
    "#     This function performs record linkage on the two dataframes using the dedupe library.\n",
    "\n",
    "#     Args:\n",
    "#         left_df (pd.DataFrame): The left dataframe.\n",
    "#         right_df (pd.DataFrame): The right dataframe.\n",
    "#         settings (str): The path to the settings file.\n",
    "#         training (str): The path to the training file.\n",
    "#         write (bool): Indicates whether to write the deduplicated output to file.\n",
    "#         out (str): The path to the output file.\n",
    "#     Returns:\n",
    "#         merged_df (pd.DataFrame): The merged dataframe.\n",
    "#     \"\"\"\n",
    "#     print('Importing data ...')\n",
    "#     if isinstance(left_df, str):\n",
    "#         root_l_df = pd.read_csv(left_df)\n",
    "#         root_r_df = pd.read_csv(right_df)\n",
    "#     else:\n",
    "#         root_l_df = left_df.copy()\n",
    "#         root_r_df = right_df.copy()\n",
    "\n",
    "#     # Stage 1: Direct products_and_services linkage using merging\n",
    "#     print(\"----Start of stage 1----\")\n",
    "#     print('Directly merging data...')\n",
    "#     # Merge the two dataframes based on the 'products_and_services' column\n",
    "#     merged_df = root_l_df.merge(root_r_df, on='products_and_services', how='left', suffixes=['_x', '_y']).drop(columns=\"ID\")\n",
    "#     merged_df = merged_df.merge(root_r_df, left_on='products_id_y', right_on='products_id', how=\"left\").drop(columns=[\"ID\",\"products_id\"])\n",
    "#     # Create a new dataframe that contains rows from company_based_p_and_s that could not be directly matched\n",
    "#     non_matched_products = merged_df[merged_df.isna().any(axis=1)].drop(columns=[\"products_id_y\", \"products_and_services_y\"]).rename(columns={\"products_and_services_x\": \"products_and_services\"})\n",
    "#     # Get the percentage of products_and_services that could be directly matched\n",
    "#     percentage_matched = len(merged_df.dropna())/len(root_l_df)*100\n",
    "#     print('Percentage of products_and_services that could be directly matched: {0:.2f}%'.format(percentage_matched))\n",
    "#     print(\"----Finished stage 1----\\n\")\n",
    "\n",
    "#     # Stage 2: Remaining products_and_services linkage using dedupe\n",
    "#     print(\"----Start of stage 2----\")\n",
    "#     print('Preparing record linkage data...')\n",
    "#     # Convert the dataframes to dictionaries\n",
    "#     linkage_data_1 = convert_pandas_to_dict(non_matched_products, \"left\", \"linkage\")\n",
    "#     linkage_data_2 = convert_pandas_to_dict(root_r_df, \"right\", \"linkage\")\n",
    "#     print('Attempting products_and_services linkage on the remainder using dedupe...')\n",
    "#     # Check if a settings file already exists and use if can be found\n",
    "#     if os.path.exists(settings):\n",
    "#         print('Settings file found! Reading settings from \"{}\"'.format(settings))\n",
    "#         with open(settings, 'rb') as sf:\n",
    "#             linker = dedupe.StaticRecordLink(sf)\n",
    "#     # If no settings file exists, create train a new linker object\n",
    "#     else:\n",
    "#         # Define the fields that will be used for the record linkage\n",
    "#         fields = [\n",
    "#                 {'field': 'products_and_services', 'type': 'String'}] # consider Text type instead of String\n",
    "        \n",
    "#         # Create a new linker object and pass the fields to it\n",
    "#         linker = dedupe.RecordLink(fields)\n",
    "#         print(\"Preparing training...\")\n",
    "#         if os.path.exists(training):\n",
    "#             print('Reading labeled examples from ', training)\n",
    "#             with open(training) as tf:\n",
    "#                 linker.prepare_training(linkage_data_1,\n",
    "#                                         linkage_data_2,\n",
    "#                                         training_file=tf,\n",
    "#                                         sample_size=10000)\n",
    "#         else:\n",
    "#             # Prepare the linker object for training using the two datasets\n",
    "#             linker.prepare_training(linkage_data_1, linkage_data_2, sample_size=10000)\n",
    "#         # Start the active labeling\n",
    "#         print('Starting active labeling...')\n",
    "#         dedupe.console_label(linker)\n",
    "#         # Train the linker object using the active labeling as additional input\n",
    "#         print(\"Training...\")\n",
    "#         linker.train()\n",
    "#         print(\"Training finished!\")\n",
    "#         # write the labelled training examples to disk\n",
    "#         with open(training, 'w') as tf:\n",
    "#             linker.write_training(tf)\n",
    "#         # write the settings file to disk\n",
    "#         with open(settings, 'wb') as sf:\n",
    "#             linker.write_settings(sf)\n",
    "#     # Perform the record linkage\n",
    "#     print('Performing linking...')\n",
    "#     linked_records = linker.join(linkage_data_1, linkage_data_2, 0.0)\n",
    "#     print('Succesfully linked {} records'.format(len(linked_records)))\n",
    "#     for _, (cluster, score) in enumerate(linked_records):\n",
    "#         non_matched_products.loc[int(re.search(r\"\\d+\", cluster[0]).group()), 'products_and_services_y'] = root_r_df.loc[int(re.search(r\"\\d+\", cluster[1]).group()), 'products_and_services']\n",
    "#         non_matched_products.loc[int(re.search(r\"\\d+\", cluster[0]).group()), 'products_id_y'] = root_r_df.loc[int(re.search(r\"\\d+\", cluster[1]).group()), 'products_id']\n",
    "    \n",
    "#     merged_df = merged_df.fillna(non_matched_products)\n",
    "#     merged_df = merged_df.rename(columns = {\"products_id_x\": \"products_id\", \n",
    "#                                                                          \"products_and_services_x\": \"automatic_processed_products_and_services\",\n",
    "#                                                                          \"products_id_y\": \"linked_EP_products_id\",\n",
    "#                                                                          \"products_and_services_y\": \"linked_EP_products_and_services\"})\n",
    "#     print(\"Coverage increased to {0:.2f}%\".format(len(merged_df.dropna())/len(root_l_df)*100))\n",
    "#     print(\"----Finished stage 2----\\n\")\n",
    "#     if write:\n",
    "#         print('Writing results to \"{}\"'.format(out))\n",
    "#         merged_df.to_csv(out, index=False)\n",
    "\n",
    "#     return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dedup_and_link(df, ep_df_path, out, dedup_settings_file, dedup_training_file, linking_settings_file, linking_training_file):\n",
    "#     # Start timer\n",
    "#     print(\"\\n/=========== Dedup x Record Linkage started ===========/\")\n",
    "#     start_time = time.time()\n",
    "#     # Phase 1: applying deduplication module to the data\n",
    "#     print(\"/=========== Start of phase 1: Deduplication ===========/\")\n",
    "#     deduped_data = deduplication(df, dedup_settings_file, dedup_training_file)\n",
    "\n",
    "#     # Phase 2: applying record linkage module to the data\n",
    "#     print(\"\\n\\n/=========== Start of phase 2: Record Linkage ===========/\")\n",
    "#     linked_data = record_linkage(deduped_data, pd.read_csv(ep_df_path), linking_settings_file, linking_training_file)\n",
    "#     end_time = time.time()\n",
    "\n",
    "#     print(\"\\n/=========== Dedup x Record Linkage finished. Duration: {} hours, {} minutes, {} seconds ===========/\".format(*seconds_conversion(end_time - start_time)))\n",
    "\n",
    "#     print('Writing results to \"{}\"'.format(out))\n",
    "#     linked_data.to_csv(out, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.1 Duplicate removal and Europages linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dedup_and_link(base_data_file_location, ep_catalogue, base_data_output_file, dedup_settings_file, dedup_training_file, rl_settings_file, rl_training_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data ...\n",
      "----Start of stage 1----\n",
      "Preparing dedupe data ...\n",
      "Settings file found! Reading settings from \"../../dedupe_files/dedup_learned_settings\"\n",
      "Clustering..\n",
      "Clustering finished!. 12864 duplicates found\n",
      "Dropping duplicates...\n",
      "Duplicates dropped!\n",
      "----Finished stage 1----\n",
      "Writing deduplicated output to file...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>products_id</th>\n",
       "      <th>raw_products_and_services</th>\n",
       "      <th>manual_processed_products_and_services</th>\n",
       "      <th>automatic_processed_products_and_services</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eab0ff0314af76111ba6cd1dea1e1f71b0b49a45</td>\n",
       "      <td>hummus</td>\n",
       "      <td>hummus</td>\n",
       "      <td>hummus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0184628897818527ff8610ee5e277c042d54bd78</td>\n",
       "      <td>magnetic platens</td>\n",
       "      <td>magnetic platens</td>\n",
       "      <td>magnetic platens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b5da318747baea08381c0f76f43061621540c5f8</td>\n",
       "      <td>platens</td>\n",
       "      <td>platens</td>\n",
       "      <td>platens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ca2549042092c61e3ab52e26228440f1f26ba6ad</td>\n",
       "      <td>germany calculations for nods and compressor g...</td>\n",
       "      <td>germany calculations for nods</td>\n",
       "      <td>Many calculations for nods and compressor grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ca2549042092c61e3ab52e26228440f1f26ba6ad</td>\n",
       "      <td>germany calculations for nods and compressor g...</td>\n",
       "      <td>germany calculations for nods</td>\n",
       "      <td>germany calculations for nods and compressor g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33000</th>\n",
       "      <td>71576cb159bee5ed36bdec2f240e74b75f60b18e</td>\n",
       "      <td>air-conditioning systems, vehicles</td>\n",
       "      <td>air-conditioning systems for vehicles</td>\n",
       "      <td>air-conditioning systems, vehicles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33001</th>\n",
       "      <td>c5d893093bc7070c63c19110a4fc0e4a265031ca</td>\n",
       "      <td>air conditioning systems for building</td>\n",
       "      <td>air conditioning systems for building</td>\n",
       "      <td>air conditioning systems for building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33002</th>\n",
       "      <td>8dd049875009bb697ad73f715bfa881b5b664bcc</td>\n",
       "      <td>air conditioning and ventilation systems for b...</td>\n",
       "      <td>air conditioning systems for building</td>\n",
       "      <td>air conditioning and ventilation systems for b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33003</th>\n",
       "      <td>a7bada8d0c2c93f0e80c8ab2226ba7394ff45ff8</td>\n",
       "      <td>cnc turned plastic parts</td>\n",
       "      <td>cnc turned plastic part</td>\n",
       "      <td>Cnc turned plastic parts.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33004</th>\n",
       "      <td>a286363727b21526cd3b3a89c6ff9b5f182fc1d1</td>\n",
       "      <td>plastic cnc turned parts</td>\n",
       "      <td>cnc turned plastic part</td>\n",
       "      <td>Plastic cnc turned parts.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33005 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    products_id  \\\n",
       "0      eab0ff0314af76111ba6cd1dea1e1f71b0b49a45   \n",
       "1      0184628897818527ff8610ee5e277c042d54bd78   \n",
       "2      b5da318747baea08381c0f76f43061621540c5f8   \n",
       "3      ca2549042092c61e3ab52e26228440f1f26ba6ad   \n",
       "4      ca2549042092c61e3ab52e26228440f1f26ba6ad   \n",
       "...                                         ...   \n",
       "33000  71576cb159bee5ed36bdec2f240e74b75f60b18e   \n",
       "33001  c5d893093bc7070c63c19110a4fc0e4a265031ca   \n",
       "33002  8dd049875009bb697ad73f715bfa881b5b664bcc   \n",
       "33003  a7bada8d0c2c93f0e80c8ab2226ba7394ff45ff8   \n",
       "33004  a286363727b21526cd3b3a89c6ff9b5f182fc1d1   \n",
       "\n",
       "                               raw_products_and_services  \\\n",
       "0                                                 hummus   \n",
       "1                                       magnetic platens   \n",
       "2                                                platens   \n",
       "3      germany calculations for nods and compressor g...   \n",
       "4      germany calculations for nods and compressor g...   \n",
       "...                                                  ...   \n",
       "33000                 air-conditioning systems, vehicles   \n",
       "33001              air conditioning systems for building   \n",
       "33002  air conditioning and ventilation systems for b...   \n",
       "33003                           cnc turned plastic parts   \n",
       "33004                           plastic cnc turned parts   \n",
       "\n",
       "      manual_processed_products_and_services  \\\n",
       "0                                     hummus   \n",
       "1                           magnetic platens   \n",
       "2                                    platens   \n",
       "3              germany calculations for nods   \n",
       "4              germany calculations for nods   \n",
       "...                                      ...   \n",
       "33000  air-conditioning systems for vehicles   \n",
       "33001  air conditioning systems for building   \n",
       "33002  air conditioning systems for building   \n",
       "33003                cnc turned plastic part   \n",
       "33004                cnc turned plastic part   \n",
       "\n",
       "               automatic_processed_products_and_services  \n",
       "0                                                 hummus  \n",
       "1                                       magnetic platens  \n",
       "2                                                platens  \n",
       "3      Many calculations for nods and compressor grou...  \n",
       "4      germany calculations for nods and compressor g...  \n",
       "...                                                  ...  \n",
       "33000                 air-conditioning systems, vehicles  \n",
       "33001              air conditioning systems for building  \n",
       "33002  air conditioning and ventilation systems for b...  \n",
       "33003                          Cnc turned plastic parts.  \n",
       "33004                          Plastic cnc turned parts.  \n",
       "\n",
       "[33005 rows x 4 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deduplication(base_data_file_location, dedup_settings_file, dedup_training_file, write = True, out = base_data_deduped_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new Italy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dedup_and_link(italy_data_file_location, ep_catalogue, italy_output_file, dedup_settings_file, dedup_training_file, rl_settings_file, rl_training_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data ...\n",
      "----Start of stage 1----\n",
      "Preparing dedupe data ...\n",
      "Settings file found! Reading settings from \"../../dedupe_files/dedup_learned_settings\"\n",
      "Clustering..\n",
      "Clustering finished!. 761 duplicates found\n",
      "Dropping duplicates...\n",
      "Duplicates dropped!\n",
      "----Finished stage 1----\n",
      "Writing deduplicated output to file...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_products_and_services</th>\n",
       "      <th>products_id</th>\n",
       "      <th>automatic_processed_products_and_services</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>persian blue salt</td>\n",
       "      <td>50abde66-58b7-4fb3-a007-1077fa41a010</td>\n",
       "      <td>Persian blue salt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>organic saffron bio</td>\n",
       "      <td>0ea739bc-c3fb-4476-b8d6-96ba5085aa00</td>\n",
       "      <td>organic saffron bio.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>property sale and purchase</td>\n",
       "      <td>126e3677-bb68-481d-96e0-ae0fd8a68db3</td>\n",
       "      <td>Property sale and purchase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tomato processing machines</td>\n",
       "      <td>46b2f19e-f238-4b99-be02-d0fbe46d1e4c</td>\n",
       "      <td>Tomato processing machines.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>christmas gift packages</td>\n",
       "      <td>6ced5115-29ba-4477-87e9-93699dce44c9</td>\n",
       "      <td>Christmas gift packages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4021</th>\n",
       "      <td>pile turner</td>\n",
       "      <td>6762bafd-8627-4894-a113-686adbc2602a</td>\n",
       "      <td>pile turner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4024</th>\n",
       "      <td>synthetic fibre yarns</td>\n",
       "      <td>856223ec-54f2-44ec-ab37-3c438164fa6b</td>\n",
       "      <td>synthetic fibre yarns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4026</th>\n",
       "      <td>polyester microfibre</td>\n",
       "      <td>2d10565b-21f6-4687-b49c-cf06e0fedabd</td>\n",
       "      <td>polyester microfibre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4027</th>\n",
       "      <td>semi-mat polyester</td>\n",
       "      <td>94cc399d-e957-4173-b192-185c7b7c82f5</td>\n",
       "      <td>semi-mat polyester</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4039</th>\n",
       "      <td>bookstore</td>\n",
       "      <td>0bb3c167-f281-404a-b16e-a190c8e9be4b</td>\n",
       "      <td>bookstore</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3512 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       raw_products_and_services                           products_id  \\\n",
       "0              persian blue salt  50abde66-58b7-4fb3-a007-1077fa41a010   \n",
       "1            organic saffron bio  0ea739bc-c3fb-4476-b8d6-96ba5085aa00   \n",
       "2     property sale and purchase  126e3677-bb68-481d-96e0-ae0fd8a68db3   \n",
       "3     tomato processing machines  46b2f19e-f238-4b99-be02-d0fbe46d1e4c   \n",
       "4        christmas gift packages  6ced5115-29ba-4477-87e9-93699dce44c9   \n",
       "...                          ...                                   ...   \n",
       "4021                 pile turner  6762bafd-8627-4894-a113-686adbc2602a   \n",
       "4024       synthetic fibre yarns  856223ec-54f2-44ec-ab37-3c438164fa6b   \n",
       "4026        polyester microfibre  2d10565b-21f6-4687-b49c-cf06e0fedabd   \n",
       "4027          semi-mat polyester  94cc399d-e957-4173-b192-185c7b7c82f5   \n",
       "4039                   bookstore  0bb3c167-f281-404a-b16e-a190c8e9be4b   \n",
       "\n",
       "     automatic_processed_products_and_services  \n",
       "0                           Persian blue salt.  \n",
       "1                         organic saffron bio.  \n",
       "2                   Property sale and purchase  \n",
       "3                  Tomato processing machines.  \n",
       "4                      Christmas gift packages  \n",
       "...                                        ...  \n",
       "4021                               pile turner  \n",
       "4024                     synthetic fibre yarns  \n",
       "4026                      polyester microfibre  \n",
       "4027                        semi-mat polyester  \n",
       "4039                                 bookstore  \n",
       "\n",
       "[3512 rows x 3 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deduplication(italy_data_file_location, dedup_settings_file, dedup_training_file, write = True, out = italy_deduped_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
