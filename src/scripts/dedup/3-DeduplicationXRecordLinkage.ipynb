{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Import the required libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "import datetime\n",
    "import dedupe\n",
    "import re\n",
    "import os\n",
    "# import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mandatory input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_catalogue= \"../../data/example_data/input/scraped_data/scraped_EP_products_catalogue.csv\"\n",
    "dedup_settings_file = '../../dedupe_files/dedup_learned_settings'\n",
    "dedup_training_file = '../../dedupe_files/dedup_training.json'\n",
    "rl_settings_file = '../../dedupe_files/record_linkage_learned_settings'\n",
    "rl_training_file = '../../dedupe_files/record_linkage_training.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_file_location =\"../../data/example_data/output/base_data/base_translated_products.csv\"\n",
    "base_data_output_file = '../../data/example_data/output/base_data/base_linked_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new Italy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "italy_data_file_location = \"../../data/example_data/output/italy_data/italy_translated_products.csv\"\n",
    "italy_deduped_file = \"../../data/example_data/output/italy_data/italy_deduped.csv\"\n",
    "italy_output_file = '../../data/example_data/output/italy_data/italy_linked_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(column):\n",
    "    column = unidecode(column)\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    column = re.sub('-', '', column)\n",
    "    column = re.sub('/', ' ', column)\n",
    "    column = re.sub(\"'\", '', column)\n",
    "    column = re.sub(\",\", '', column)\n",
    "    column = re.sub(\":\", ' ', column)\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    if not column:\n",
    "        column = None\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pandas_to_dict(dataframe, which = \"None\", type = \"dedup\"):\n",
    "    data_d = {}\n",
    "    for i, row in dataframe.iterrows():\n",
    "        x = zip(row.index, row.values)\n",
    "        clean_row = dict([(k, preProcess(str(v))) for (k, v) in x])\n",
    "        if type != \"dedup\":\n",
    "            data_d[which + str(i)] = clean_row\n",
    "        else:\n",
    "            data_d[i] = dict(clean_row)\n",
    "    return data_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seconds_conversion(seconds):\n",
    "    # Convert the time difference to a timedelta object\n",
    "    time_delta = datetime.timedelta(seconds=seconds)\n",
    "\n",
    "    # Extract the hours, minutes, and seconds from the timedelta object\n",
    "    hours = time_delta.seconds // 3600\n",
    "    minutes = (time_delta.seconds % 3600) // 60\n",
    "    seconds = time_delta.seconds % 60\n",
    "    return (hours, minutes, seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dedupe modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deduplication module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplication(file, settings, training, write = False, out = \"None\"):\n",
    "   # Read the csv files\n",
    "   print('Importing data ...')\n",
    "   df = pd.read_csv(file)\n",
    "\n",
    "   # stage 1: Deduplication using dedupe library\n",
    "   print(\"----Start of stage 1----\")\n",
    "   print('Preparing dedupe data ...')\n",
    "   dedup_data = convert_pandas_to_dict(df, \"dedup\")\n",
    "\n",
    "   if os.path.exists(settings):\n",
    "      print('Settings file found! Reading settings from \"{}\"'.format(settings))\n",
    "      with open(settings, 'rb') as sf:\n",
    "         deduper = dedupe.StaticDedupe(sf)\n",
    "   # If no settings file exists, create train a new linker object\n",
    "   else:\n",
    "      # Define the fields that will be used for the record linkage\n",
    "      fields = [\n",
    "               {'field': 'products_and_services', 'type': 'String'}] # consider Text type instead of String\n",
    "      \n",
    "      # Create a new linker object and pass the fields to it\n",
    "      deduper = dedupe.Dedupe(fields)\n",
    "      print(\"Preparing training...\")\n",
    "      if os.path.exists(training):\n",
    "         print('Reading labeled examples from ', training)\n",
    "         with open(training) as tf:\n",
    "               deduper.prepare_training(dedup_data,\n",
    "                                       training_file=tf)\n",
    "      else:\n",
    "         # Prepare the linker object for training using the two datasets\n",
    "         deduper.prepare_training(dedup_data)\n",
    "      # Start the active labeling\n",
    "      print('Starting active labeling...')\n",
    "      dedupe.console_label(deduper)\n",
    "      # Train the linker object using the active labeling as additional input\n",
    "      print(\"Training...\")\n",
    "      deduper.train()\n",
    "      print(\"Training finished!\")\n",
    "      # write the labelled training examples to disk\n",
    "      with open(training, 'w') as tf:\n",
    "         deduper.write_training(tf)\n",
    "      # write the settings file to disk\n",
    "      with open(settings, 'wb') as sf:\n",
    "         deduper.write_settings(sf)\n",
    "\n",
    "   print('Clustering..')\n",
    "   clustered_dupes = deduper.partition(dedup_data, 0.5)\n",
    "   print('Clustering finished!. {} duplicates found'.format(len(df)-len(clustered_dupes)))\n",
    "\n",
    "   print('Dropping duplicates...')\n",
    "   rows_to_drop = []\n",
    "   for _, (records, scores) in enumerate(clustered_dupes):\n",
    "      rows_to_drop.append(records[1:])\n",
    "\n",
    "   # flatten list of lists\n",
    "   rows_to_drop = [item for sublist in rows_to_drop for item in sublist]\n",
    "   df = df.drop(df.index[rows_to_drop])\n",
    "   \n",
    "   print (\"Duplicates dropped!\")\n",
    "   print(\"----Finished stage 1----\")\n",
    "   \n",
    "   if write: \n",
    "      print('Writing deduplicated output to file...')\n",
    "      df.to_csv(out, index=False)\n",
    "\n",
    "   return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record Linkage module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_linkage(left_df, right_df, settings, training, write = False, out = \"None\"):\n",
    "    root_l_df = left_df.copy()\n",
    "    root_r_df = right_df.copy()\n",
    "    if isinstance(left_df, str):\n",
    "        print('Importing data ...')\n",
    "        root_l_df = pd.read_csv(left_df)\n",
    "        root_r_df = pd.read_csv(right_df)\n",
    "\n",
    "    # Stage 1: Direct products_and_services linkage using merging\n",
    "    print(\"----Start of stage 1----\")\n",
    "    print('Directly merging data...')\n",
    "    # Merge the two dataframes based on the 'products_and_services' column\n",
    "    merged_df = root_l_df.merge(root_r_df, on='products_and_services', how='left', suffixes=['_x', '_y']).drop(columns=\"ID\")\n",
    "    merged_df = merged_df.merge(root_r_df, left_on='products_id_y', right_on='products_id', how=\"left\").drop(columns=[\"ID\",\"products_id\"])\n",
    "    # Create a new dataframe that contains rows from company_based_p_and_s that could not be directly matched\n",
    "    non_matched_products = merged_df[merged_df.isna().any(axis=1)].drop(columns=[\"products_id_y\", \"products_and_services_y\"]).rename(columns={\"products_and_services_x\": \"products_and_services\"})\n",
    "    # Get the percentage of products_and_services that could be directly matched\n",
    "    percentage_matched = len(merged_df.dropna())/len(root_l_df)*100\n",
    "    print('Percentage of products_and_services that could be directly matched: {0:.2f}%'.format(percentage_matched))\n",
    "    print(\"----Finished stage 1----\\n\")\n",
    "\n",
    "    # Stage 2: Remaining products_and_services linkage using dedupe\n",
    "    print(\"----Start of stage 2----\")\n",
    "    print('Preparing record linkage data...')\n",
    "    # Convert the dataframes to dictionaries\n",
    "    linkage_data_1 = convert_pandas_to_dict(non_matched_products, \"left\", \"linkage\")\n",
    "    linkage_data_2 = convert_pandas_to_dict(root_r_df, \"right\", \"linkage\")\n",
    "    print('Attempting products_and_services linkage on the remainder using dedupe...')\n",
    "    # Check if a settings file already exists and use if can be found\n",
    "    if os.path.exists(settings):\n",
    "        print('Settings file found! Reading settings from \"{}\"'.format(settings))\n",
    "        with open(settings, 'rb') as sf:\n",
    "            linker = dedupe.StaticRecordLink(sf)\n",
    "    # If no settings file exists, create train a new linker object\n",
    "    else:\n",
    "        # Define the fields that will be used for the record linkage\n",
    "        fields = [\n",
    "                {'field': 'products_and_services', 'type': 'String'}] # consider Text type instead of String\n",
    "        \n",
    "        # Create a new linker object and pass the fields to it\n",
    "        linker = dedupe.RecordLink(fields)\n",
    "        print(\"Preparing training...\")\n",
    "        if os.path.exists(training):\n",
    "            print('Reading labeled examples from ', training)\n",
    "            with open(training) as tf:\n",
    "                linker.prepare_training(linkage_data_1,\n",
    "                                        linkage_data_2,\n",
    "                                        training_file=tf,\n",
    "                                        sample_size=10000)\n",
    "        else:\n",
    "            # Prepare the linker object for training using the two datasets\n",
    "            linker.prepare_training(linkage_data_1, linkage_data_2, sample_size=10000)\n",
    "        # Start the active labeling\n",
    "        print('Starting active labeling...')\n",
    "        dedupe.console_label(linker)\n",
    "        # Train the linker object using the active labeling as additional input\n",
    "        print(\"Training...\")\n",
    "        linker.train()\n",
    "        print(\"Training finished!\")\n",
    "        # write the labelled training examples to disk\n",
    "        with open(training, 'w') as tf:\n",
    "            linker.write_training(tf)\n",
    "        # write the settings file to disk\n",
    "        with open(settings, 'wb') as sf:\n",
    "            linker.write_settings(sf)\n",
    "    # Perform the record linkage\n",
    "    print('Performing linking...')\n",
    "    linked_records = linker.join(linkage_data_1, linkage_data_2, 0.0)\n",
    "    print('Succesfully linked {} records'.format(len(linked_records)))\n",
    "    for _, (cluster, score) in enumerate(linked_records):\n",
    "        non_matched_products.loc[int(re.search(r\"\\d+\", cluster[0]).group()), 'products_and_services_y'] = root_r_df.loc[int(re.search(r\"\\d+\", cluster[1]).group()), 'products_and_services']\n",
    "        non_matched_products.loc[int(re.search(r\"\\d+\", cluster[0]).group()), 'products_id_y'] = root_r_df.loc[int(re.search(r\"\\d+\", cluster[1]).group()), 'products_id']\n",
    "    \n",
    "    merged_df = merged_df.fillna(non_matched_products).rename(columns = {\"products_and_services_y\": \"EuroPages products_and_services\", \"products_id_y\": \"EuroPages products_id\"})\n",
    "    print(\"Coverage increased to {0:.2f}%\".format(len(merged_df.dropna())/len(root_l_df)*100))\n",
    "    print(\"----Finished stage 2----\\n\")\n",
    "    if write:\n",
    "        print('Writing results to \"{}\"'.format(out))\n",
    "        merged_df.to_csv(out, index=False)\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup_and_link(df, ep_df_path, out, dedup_settings_file, dedup_training_file, linking_settings_file, linking_training_file):\n",
    "    # Start timer\n",
    "    print(\"\\n/=========== Dedup x Record Linkage started ===========/\")\n",
    "    start_time = time.time()\n",
    "    # Phase 1: applying deduplication module to the data\n",
    "    print(\"/=========== Start of phase 1: Deduplication ===========/\")\n",
    "    deduped_data = deduplication(df, dedup_settings_file, dedup_training_file)\n",
    "\n",
    "    # Phase 2: applying record linkage module to the data\n",
    "    print(\"\\n\\n/=========== Start of phase 2: Record Linkage ===========/\")\n",
    "    linked_data = record_linkage(deduped_data, pd.read_csv(ep_df_path), linking_settings_file, linking_training_file)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(\"\\n/=========== Dedup x Record Linkage finished. Duration: {} hours, {} minutes, {} seconds ===========/\".format(*seconds_conversion(end_time - start_time)))\n",
    "\n",
    "    print('Writing results to \"{}\"'.format(out))\n",
    "    linked_data.to_csv(out, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.1 Duplicate removal and Europages linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/=========== Dedup x Record Linkage started ===========/\n",
      "/=========== Start of phase 1: Deduplication ===========/\n",
      "Importing data ...\n",
      "----Start of stage 1----\n",
      "Preparing dedupe data ...\n",
      "Settings file found! Reading settings from \"../../dedupe_files/dedup_learned_settings\"\n",
      "Clustering..\n",
      "Clustering finished!. 99 duplicates found\n",
      "Dropping duplicates...\n",
      "Duplicates dropped!\n",
      "----Finished stage 1----\n",
      "\n",
      "\n",
      "/=========== Start of phase 2: Record Linkage ===========/\n",
      "----Start of stage 1----\n",
      "Directly merging data...\n",
      "Percentage of products_and_services that could be directly matched: 29.17%\n",
      "----Finished stage 1----\n",
      "\n",
      "----Start of stage 2----\n",
      "Preparing record linkage data...\n",
      "Attempting products_and_services linkage on the remainder using dedupe...\n",
      "Settings file found! Reading settings from \"../../dedupe_files/record_linkage_learned_settings\"\n",
      "Performing linking...\n",
      "Succesfully linked 2875 records\n",
      "Coverage increased to 43.20%\n",
      "----Finished stage 2----\n",
      "\n",
      "\n",
      "\n",
      "/=========== Dedup x Record Linkage finished. Duration: 0 hours, 0 minutes, 30 seconds ===========/\n",
      "Writing results to \"../../data/example_data/output/base_data/base_linked_data.csv\"\n"
     ]
    }
   ],
   "source": [
    "dedup_and_link(base_data_file_location, ep_catalogue, base_data_output_file, dedup_settings_file, dedup_training_file, rl_settings_file, rl_training_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new Italy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/=========== Dedup x Record Linkage started ===========/\n",
      "/=========== Start of phase 1: Deduplication ===========/\n",
      "Importing data ...\n",
      "----Start of stage 1----\n",
      "Preparing dedupe data ...\n",
      "Settings file found! Reading settings from \"../../dedupe_files/dedup_learned_settings\"\n",
      "Clustering..\n",
      "Clustering finished!. 3 duplicates found\n",
      "Dropping duplicates...\n",
      "Duplicates dropped!\n",
      "----Finished stage 1----\n",
      "\n",
      "\n",
      "/=========== Start of phase 2: Record Linkage ===========/\n",
      "----Start of stage 1----\n",
      "Directly merging data...\n",
      "Percentage of products_and_services that could be directly matched: 57.90%\n",
      "----Finished stage 1----\n",
      "\n",
      "----Start of stage 2----\n",
      "Preparing record linkage data...\n",
      "Attempting products_and_services linkage on the remainder using dedupe...\n",
      "Settings file found! Reading settings from \"../../dedupe_files/record_linkage_learned_settings\"\n",
      "Performing linking...\n",
      "Succesfully linked 554 records\n",
      "Coverage increased to 74.04%\n",
      "----Finished stage 2----\n",
      "\n",
      "\n",
      "\n",
      "/=========== Dedup x Record Linkage finished. Duration: 0 hours, 0 minutes, 23 seconds ===========/\n",
      "Writing results to \"../../data/example_data/output/italy_data/italy_linked_data.csv\"\n"
     ]
    }
   ],
   "source": [
    "dedup_and_link(italy_data_file_location, ep_catalogue, italy_output_file, dedup_settings_file, dedup_training_file, rl_settings_file, rl_training_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
