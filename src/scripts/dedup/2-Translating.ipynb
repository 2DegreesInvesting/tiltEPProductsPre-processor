{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a71753e7-6deb-45c3-a226-af7cdec0e394",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2. Translate the input products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2447feb3-753a-4e72-a72e-0bf689a7cac8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2.1 Import the required libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "922c33d3-4f90-4c1d-b25d-37955e860fc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from deep_translator import GoogleTranslator\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "translator = GoogleTranslator(source='auto', target='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9cdd112-f4a3-48b5-86a4-3b092c7c31cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2.2 Load in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d3a5cf7-8184-4659-8799-18ac5e24cd85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if 'DATABRICKS_RUNTIME_VERSION' in os.environ:\n",
    "    # Obtain the file from the Azure Cloud Container\n",
    "    base_file_location = \"abfss://preprocessing@storagetiltdevelop.dfs.core.windows.net/data/example_data/output/base_data/base_typo_corrected_products.csv\"\n",
    "    # Obtain the directory of the new Italy data\n",
    "    italy_file_location = \"abfss://preprocessing@storagetiltdevelop.dfs.core.windows.net/data/example_data/output/italy_data/italy_typo_corrected_products.csv\"\n",
    "\n",
    "    # Read the data into a DataFrame\n",
    "    base_corrected_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\",\"true\").csv(base_file_location).toPandas()\n",
    "    # Read the data into a DataFrame\n",
    "    italy_corrected_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\",\"true\").csv(italy_file_location).toPandas()\n",
    "\n",
    "else:\n",
    "    # Obtain the file from the Azure Cloud Container\n",
    "    base_file_location = \"../../data/example_data/output/base_data/base_typo_corrected_products.csv\"\n",
    "    # Obtain the directory of the new Italy data\n",
    "    italy_file_location = \"../../data/example_data/output/italy_data/italy_typo_corrected_products.csv\"\n",
    "\n",
    "    # Read the data into a DataFrame\n",
    "    base_corrected_df = pd.read_csv(base_file_location)\n",
    "    # Read the data into a DataFrame\n",
    "    italy_corrected_df = pd.read_csv(italy_file_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab154959-14fe-4150-8b21-467a633136c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2.3 Translate dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bb764a8-3990-4347-9b21-9ca80b2035c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4932fbc8-dc29-45b1-b08a-4ba1882d83af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def translate_Google(text):\n",
    "    \"\"\"\n",
    "    This function translates the text into English using Google Translator\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        translated = translator.translate(text)\n",
    "        return translated\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9ea4f3f-49ab-4b3d-9703-1192f873a3f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Google translator module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5516abcd-aac3-40e3-b150-54387c63369e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def translate_df(df):\n",
    "    \"\"\"\n",
    "    This function translates the dataframe into English using Google Translator\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to be translated.\n",
    "    Returns:\n",
    "        translated_df (pd.DataFrame): The translated dataframe.\n",
    "    \"\"\"\n",
    "    # then take subset of english texts\n",
    "    print(\"Taking subset of non-english texts...\")\n",
    "    # filter out non-english texts and text that do not have a language code\n",
    "    non_english_df = df[(df[\"language (ISO-code)\"].isnull() == False) & (df[\"language (ISO-code)\"] != \"en\")]\n",
    "    # exclude the rows from non_english_df from the original df\n",
    "    df = df[~df.index.isin(non_english_df.index)]\n",
    "\n",
    "    # apply typo correction to english texts\n",
    "    print(\"Applying translation...\")\n",
    "    non_english_df = non_english_df.copy()\n",
    "    non_english_df.loc[:, 'translated_text'] = non_english_df['typo_corrected'].progress_apply(translate_Google)\n",
    "\n",
    "    # merge the corrected english texts with the original df\n",
    "    print(\"Merging the corrected english texts with the original df...\\n\")\n",
    "    df = pd.concat([df, non_english_df], ignore_index=True)\n",
    "    \n",
    "    # replace empty values in translated column with the typo corrected text\n",
    "    df[\"translated_text\"].fillna(df[\"typo_corrected\"], inplace=True)\n",
    "    translated_df = df.copy().drop(columns=[\"typo_corrected\", \"language (ISO-code)\", \"to_process\"]).rename(columns={\"products_and_services\":\"raw_products_and_services\",\"translated_text\": \"products_and_services\"})\n",
    "    return translated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69a974be-53f3-43e9-ad5e-d71341f26ecb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Base Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3b62dd3-fac2-42ee-98a8-076d6100fdae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_translated_df = translate_df(base_corrected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d21c988-ebea-4fcb-b433-39dd04a0e235",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### new Italy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "976aa595-fb54-4e22-bea3-3c4f46cc5a80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "italy_translated_df = translate_df(italy_corrected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ed9ea12-92bb-4357-9185-27525cd63d3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2.4 Export the dataframe with the translated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b858cbb2-23f9-4249-9a52-dc3d6a281d89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if 'DATABRICKS_RUNTIME_VERSION' in os.environ:\n",
    "    # Define the path for the new dataframe\n",
    "    output_path_base_translated = \"abfss://preprocessing@storagetiltdevelop.dfs.core.windows.net/data/example_data/output/base_data/base_translated_products.csv\"\n",
    "    # Define the path for the new dataframe\n",
    "    output_path_italy_translated = \"abfss://preprocessing@storagetiltdevelop.dfs.core.windows.net/data/example_data/output/italy_data/italy_translated_products.csv\"\n",
    "\n",
    "    # Convert the pandas dataframe to a spark sql dataframe\n",
    "    base_translated_spark = spark.createDataFrame(base_translated_df)\n",
    "    # Convert the pandas dataframe to a spark sql dataframe\n",
    "    italy_typo_corrected_spark = spark.createDataFrame(italy_translated_df)\n",
    "\n",
    "    # Write the new dataframe to the path\n",
    "    base_translated_spark.write.csv(output_path_base_translated, mode=\"overwrite\", header=True)\n",
    "    # Write the new dataframe to the path\n",
    "    italy_typo_corrected_spark.write.csv(output_path_italy_translated, mode=\"overwrite\", header=True)\n",
    "else:\n",
    "    output_path_base_translated = \"../../data/example_data/output/base_data/base_translated_products.csv\"\n",
    "    output_path_italy_translated = \"../../data/example_data/output/italy_data/italy_translated_products.csv\"\n",
    "    base_translated_df.to_csv(output_path_base_translated)\n",
    "    italy_translated_df.to_csv(output_path_italy_translated)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2-Translating",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
