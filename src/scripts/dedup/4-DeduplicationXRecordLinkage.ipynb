{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d7421ca-323e-4a4e-a690-bdb5f6ae8d08",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 4. Deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaf9e3da-43c2-43a9-a901-e246b332228d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4.1 Import the required libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "709dac03-54a8-4f68-9541-39a029b1b55d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import dedupe\n",
    "import time\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fc8d346-c735-4d37-8bff-54125b6711d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4.2 Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1396ed55-8464-4c88-9ed1-6d3710cdc25f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Mandatory input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e14522b5-e81a-4b39-9ae8-cacd0d48ff80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dedup_settings_file = '../../dedupe_files/dedup_learned_settings'\n",
    "dedup_training_file = '../../dedupe_files/dedup_training.json'\n",
    "rl_settings_file = '../../dedupe_files/record_linkage_learned_settings'\n",
    "rl_training_file = '../../dedupe_files/record_linkage_training.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65136e89-04b4-41ad-bb2b-995b230f601c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if 'DATABRICKS_RUNTIME_VERSION' in os.environ:\n",
    "    def file_exists(path):\n",
    "        try:\n",
    "            dbutils.fs.ls(path)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            if 'java.io.FileNotFoundException' in str(e):\n",
    "                raise FileNotFoundError(\"File could not be found. Are you sure the file exists in the provided directory?\")\n",
    "            else:\n",
    "                raise\n",
    "    input_path = dbutils.jobs.taskValues.get(taskKey = \"Translating\", key = \"OutPath\", default = \"None\", debugValue = 0)\n",
    "    file_exists(input_path)\n",
    "    translated_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\",\"true\").csv(input_path).toPandas()\n",
    "    temp_storage = \"abfss://preprocessing@storagetiltdevelop.dfs.core.windows.net/data/example_data/output/address-temp\"\n",
    "    deduped_df_path = str(os.path.join(os.path.dirname(os.path.dirname(input_path)), \"output/deduped_\" + os.path.basename(input_path))).replace(\"\\\\\", \"/\")    \n",
    "else:\n",
    "    ep_catalogue= \"../../data/example_data/input/scraped_data/scraped_EP_products_catalogue.csv\"\n",
    "    base_data_file_location =\"../../data/example_data/output/base_data/base_translated_products.csv\"\n",
    "    base_data_deduped_file =  \"../../data/example_data/output/base_data/base_deduped.csv\"\n",
    "    base_data_output_file = '../../data/example_data/output/base_data/base_linked_data.csv'\n",
    "    italy_data_file_location = \"../../data/example_data/output/italy_data/italy_translated_products.csv\"\n",
    "    italy_deduped_file = \"../../data/example_data/output/italy_data/italy_deduped.csv\"\n",
    "    italy_output_file = '../../data/example_data/output/italy_data/italy_linked_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62a98a24-f8d4-459b-9bef-703e62284079",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4.3 Deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0b01264-fbd4-41cf-bb02-6e251b0f6dd5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a57a50dc-dfcf-4769-82a9-d67ff5fc607c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def preProcess(column):\n",
    "    column = unidecode(column)\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    column = re.sub('-', '', column)\n",
    "    column = re.sub('/', ' ', column)\n",
    "    column = re.sub(\"'\", '', column)\n",
    "    column = re.sub(\",\", '', column)\n",
    "    column = re.sub(\":\", ' ', column)\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    if not column:\n",
    "        column = None\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06a551a5-8107-4975-8219-4b6d5a1be171",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def convert_pandas_to_dict(dataframe, which = \"None\", type = \"dedup\"):\n",
    "    data_d = {}\n",
    "    for i, row in dataframe.iterrows():\n",
    "        x = zip(row.index, row.values)\n",
    "        clean_row = dict([(k, preProcess(str(v))) for (k, v) in x])\n",
    "        if type != \"dedup\":\n",
    "            data_d[which + str(i)] = clean_row\n",
    "        else:\n",
    "            data_d[i] = dict(clean_row)\n",
    "    return data_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5a350fc-a3e1-49a1-b1c9-3743eb1c3865",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def seconds_conversion(seconds):\n",
    "    # Convert the time difference to a timedelta object\n",
    "    time_delta = datetime.timedelta(seconds=seconds)\n",
    "\n",
    "    # Extract the hours, minutes, and seconds from the timedelta object\n",
    "    hours = time_delta.seconds // 3600\n",
    "    minutes = (time_delta.seconds % 3600) // 60\n",
    "    seconds = time_delta.seconds % 60\n",
    "    return (hours, minutes, seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a3f7436-6171-493d-9622-bc02ca9a67d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Dedupe modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "672b7f66-018f-4a50-8ecb-600a3eb7ddbf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Deduplication module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d660d057-ebfe-4e24-8088-d30aad60e44b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def deduplication(file, settings, training, write = False, out = \"None\"):\n",
    "   \"\"\"\n",
    "   This function deduplicates the dataframe using the dedupe library.\n",
    "\n",
    "   Args:\n",
    "       file (str or pd.Dataframe): The path to the file to be deduplicated or a pandas dataframe.\n",
    "       settings (str): The path to the settings file.\n",
    "       training (str): The path to the training file.\n",
    "       write (bool): Indicates whether to write the deduplicated output to file.\n",
    "       out (str): The path to the output file.\n",
    "    Returns:\n",
    "        df (pd.DataFrame): The deduplicated dataframe.\n",
    "   \"\"\"\n",
    "   # Read the csv files\n",
    "   print('Importing data ...')\n",
    "   if isinstance(file, str):\n",
    "       if 'DATABRICKS_RUNTIME_VERSION' in os.environ:\n",
    "         df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(file).toPandas()\n",
    "       else:\n",
    "         df = pd.read_csv(file)\n",
    "   else:\n",
    "       df = file\n",
    "\n",
    "   # stage 1: Deduplication using dedupe library\n",
    "   print(\"----Start of stage 1----\")\n",
    "   print('Preparing dedupe data ...')\n",
    "   dedup_data = convert_pandas_to_dict(df, \"dedup\")\n",
    "   if os.path.exists(settings):\n",
    "      print('Settings file found! Reading settings from \"{}\"'.format(settings))\n",
    "      with open(settings, 'rb') as sf:\n",
    "         deduper = dedupe.StaticDedupe(sf)\n",
    "   # If no settings file exists, create train a new linker object\n",
    "   else:\n",
    "      # Define the fields that will be used for the record linkage\n",
    "      fields = [\n",
    "               {'field': 'products_and_services', 'type': 'String'}] # consider Text type instead of String\n",
    "      \n",
    "      # Create a new linker object and pass the fields to it\n",
    "      deduper = dedupe.Dedupe(fields)\n",
    "      print(\"Preparing training...\")\n",
    "      if os.path.exists(training):\n",
    "         print('Reading labeled examples from ', training)\n",
    "         with open(training) as tf:\n",
    "               deduper.prepare_training(dedup_data,\n",
    "                                       training_file=tf)\n",
    "      else:\n",
    "         # Prepare the linker object for training using the two datasets\n",
    "         deduper.prepare_training(dedup_data)\n",
    "      # Start the active labeling\n",
    "      print('Starting active labeling...')\n",
    "      dedupe.console_label(deduper)\n",
    "      # Train the linker object using the active labeling as additional input\n",
    "      print(\"Training...\")\n",
    "      deduper.train()\n",
    "      print(\"Training finished!\")\n",
    "      # write the labelled training examples to disk\n",
    "      with open(training, 'w') as tf:\n",
    "         deduper.write_training(tf)\n",
    "      # write the settings file to disk\n",
    "      with open(settings, 'wb') as sf:\n",
    "         deduper.write_settings(sf)\n",
    "\n",
    "   print('Clustering..')\n",
    "   clustered_dupes = deduper.partition(dedup_data, 0.5)\n",
    "   print('Clustering finished!. {} duplicates found'.format(len(df)-len(clustered_dupes)))\n",
    "\n",
    "   print('Dropping duplicates...')\n",
    "   rows_to_drop = []\n",
    "   for _, (records, scores) in enumerate(clustered_dupes):\n",
    "      rows_to_drop.append(records[1:])\n",
    "\n",
    "   # flatten list of lists\n",
    "   rows_to_drop = [item for sublist in rows_to_drop for item in sublist]\n",
    "   df = df.drop(df.index[rows_to_drop])\n",
    "   \n",
    "   print (\"Duplicates dropped!\")\n",
    "   print(\"----Finished stage 1----\")\n",
    "   # rename product_and_services column to automated_processed_products_and_services\n",
    "   df = df.rename(columns={'products_and_services': 'automatic_processed_products_and_services'})\n",
    "   \n",
    "   if write: \n",
    "      # if write_path string contains the word base then we are also merging it with the manual clustered data\n",
    "      if 'base' in out:\n",
    "         if 'DATABRICKS_RUNTIME_VERSION' in os.environ:\n",
    "            manual_clustered_df = spark.read.option(\"header\", \"true\").csv('abfss://preprocessing@storagetiltdevelop.dfs.core.windows.net/data/example_data/input/manual_clustering.csv').toPandas()\n",
    "         else:\n",
    "            # read in manual clustered datax \n",
    "            manual_clustered_df = pd.read_csv('../../data/example_data/input/manual_clustering.csv')\n",
    "         # merge the two dataframes on products_id\n",
    "         df = pd.merge(manual_clustered_df, df, on='products_id')\n",
    "         # drop cluster_id, clustered_delimited_id,delimited_id\n",
    "         df = df.drop(['clustered_id', 'clustered_delimited_id', 'delimited_id', 'delimited_products_id'], axis=1).rename(columns = {\"clustered\": \"manual_processed_products_and_services\"})\n",
    "         # reorder columns\n",
    "         df = df[['products_id','raw_products_and_services', 'manual_processed_products_and_services', 'automatic_processed_products_and_services']]\n",
    "      print('Writing deduplicated output to file...')\n",
    "      if 'DATABRICKS_RUNTIME_VERSION' in os.environ:\n",
    "         # Convert the pandas dataframe to a spark sql dataframe\n",
    "         df_spark = spark.createDataFrame(df)\n",
    "         \n",
    "         df_spark.coalesce(1).write.csv(\"abfss://preprocessing@storagetiltdevelop.dfs.core.windows.net/data/example_data/output/address-temp\",  mode=\"overwrite\", header=True)\n",
    "\n",
    "         ##This remove all CRC files\n",
    "         file_path = dbutils.fs.ls(\"abfss://preprocessing@storagetiltdevelop.dfs.core.windows.net/data/example_data/output/address-temp/\")\n",
    "         csv_path = [x.path for x in file_path if x.name.endswith(\".csv\")][0]\n",
    "         dbutils.fs.cp(csv_path ,out)\n",
    "         dbutils.fs.rm(\"abfss://preprocessing@storagetiltdevelop.dfs.core.windows.net/data/example_data/output/address-temp\", recurse=True)\n",
    "      else:\n",
    "         df.to_csv(out, index=False)\n",
    "      \n",
    "   return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0038b7ed-0ee7-437a-a239-bad2d0d50991",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Record Linkage module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1f07715-4a9b-4f32-b506-cdcc20e2639e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def record_linkage(left_df, right_df, settings, training, write = False, out = \"None\"):\n",
    "#     \"\"\"\n",
    "#     This function performs record linkage on the two dataframes using the dedupe library.\n",
    "\n",
    "#     Args:\n",
    "#         left_df (pd.DataFrame): The left dataframe.\n",
    "#         right_df (pd.DataFrame): The right dataframe.\n",
    "#         settings (str): The path to the settings file.\n",
    "#         training (str): The path to the training file.\n",
    "#         write (bool): Indicates whether to write the deduplicated output to file.\n",
    "#         out (str): The path to the output file.\n",
    "#     Returns:\n",
    "#         merged_df (pd.DataFrame): The merged dataframe.\n",
    "#     \"\"\"\n",
    "#     print('Importing data ...')\n",
    "#     if isinstance(left_df, str):\n",
    "#         root_l_df = pd.read_csv(left_df)\n",
    "#         root_r_df = pd.read_csv(right_df)\n",
    "#     else:\n",
    "#         root_l_df = left_df.copy()\n",
    "#         root_r_df = right_df.copy()\n",
    "\n",
    "#     # Stage 1: Direct products_and_services linkage using merging\n",
    "#     print(\"----Start of stage 1----\")\n",
    "#     print('Directly merging data...')\n",
    "#     # Merge the two dataframes based on the 'products_and_services' column\n",
    "#     merged_df = root_l_df.merge(root_r_df, on='products_and_services', how='left', suffixes=['_x', '_y']).drop(columns=\"ID\")\n",
    "#     merged_df = merged_df.merge(root_r_df, left_on='products_id_y', right_on='products_id', how=\"left\").drop(columns=[\"ID\",\"products_id\"])\n",
    "#     # Create a new dataframe that contains rows from company_based_p_and_s that could not be directly matched\n",
    "#     non_matched_products = merged_df[merged_df.isna().any(axis=1)].drop(columns=[\"products_id_y\", \"products_and_services_y\"]).rename(columns={\"products_and_services_x\": \"products_and_services\"})\n",
    "#     # Get the percentage of products_and_services that could be directly matched\n",
    "#     percentage_matched = len(merged_df.dropna())/len(root_l_df)*100\n",
    "#     print('Percentage of products_and_services that could be directly matched: {0:.2f}%'.format(percentage_matched))\n",
    "#     print(\"----Finished stage 1----\\n\")\n",
    "\n",
    "#     # Stage 2: Remaining products_and_services linkage using dedupe\n",
    "#     print(\"----Start of stage 2----\")\n",
    "#     print('Preparing record linkage data...')\n",
    "#     # Convert the dataframes to dictionaries\n",
    "#     linkage_data_1 = convert_pandas_to_dict(non_matched_products, \"left\", \"linkage\")\n",
    "#     linkage_data_2 = convert_pandas_to_dict(root_r_df, \"right\", \"linkage\")\n",
    "#     print('Attempting products_and_services linkage on the remainder using dedupe...')\n",
    "#     # Check if a settings file already exists and use if can be found\n",
    "#     if os.path.exists(settings):\n",
    "#         print('Settings file found! Reading settings from \"{}\"'.format(settings))\n",
    "#         with open(settings, 'rb') as sf:\n",
    "#             linker = dedupe.StaticRecordLink(sf)\n",
    "#     # If no settings file exists, create train a new linker object\n",
    "#     else:\n",
    "#         # Define the fields that will be used for the record linkage\n",
    "#         fields = [\n",
    "#                 {'field': 'products_and_services', 'type': 'String'}] # consider Text type instead of String\n",
    "        \n",
    "#         # Create a new linker object and pass the fields to it\n",
    "#         linker = dedupe.RecordLink(fields)\n",
    "#         print(\"Preparing training...\")\n",
    "#         if os.path.exists(training):\n",
    "#             print('Reading labeled examples from ', training)\n",
    "#             with open(training) as tf:\n",
    "#                 linker.prepare_training(linkage_data_1,\n",
    "#                                         linkage_data_2,\n",
    "#                                         training_file=tf,\n",
    "#                                         sample_size=10000)\n",
    "#         else:\n",
    "#             # Prepare the linker object for training using the two datasets\n",
    "#             linker.prepare_training(linkage_data_1, linkage_data_2, sample_size=10000)\n",
    "#         # Start the active labeling\n",
    "#         print('Starting active labeling...')\n",
    "#         dedupe.console_label(linker)\n",
    "#         # Train the linker object using the active labeling as additional input\n",
    "#         print(\"Training...\")\n",
    "#         linker.train()\n",
    "#         print(\"Training finished!\")\n",
    "#         # write the labelled training examples to disk\n",
    "#         with open(training, 'w') as tf:\n",
    "#             linker.write_training(tf)\n",
    "#         # write the settings file to disk\n",
    "#         with open(settings, 'wb') as sf:\n",
    "#             linker.write_settings(sf)\n",
    "#     # Perform the record linkage\n",
    "#     print('Performing linking...')\n",
    "#     linked_records = linker.join(linkage_data_1, linkage_data_2, 0.0)\n",
    "#     print('Succesfully linked {} records'.format(len(linked_records)))\n",
    "#     for _, (cluster, score) in enumerate(linked_records):\n",
    "#         non_matched_products.loc[int(re.search(r\"\\d+\", cluster[0]).group()), 'products_and_services_y'] = root_r_df.loc[int(re.search(r\"\\d+\", cluster[1]).group()), 'products_and_services']\n",
    "#         non_matched_products.loc[int(re.search(r\"\\d+\", cluster[0]).group()), 'products_id_y'] = root_r_df.loc[int(re.search(r\"\\d+\", cluster[1]).group()), 'products_id']\n",
    "    \n",
    "#     merged_df = merged_df.fillna(non_matched_products)\n",
    "#     merged_df = merged_df.rename(columns = {\"products_id_x\": \"products_id\", \n",
    "#                                                                          \"products_and_services_x\": \"automatic_processed_products_and_services\",\n",
    "#                                                                          \"products_id_y\": \"linked_EP_products_id\",\n",
    "#                                                                          \"products_and_services_y\": \"linked_EP_products_and_services\"})\n",
    "#     print(\"Coverage increased to {0:.2f}%\".format(len(merged_df.dropna())/len(root_l_df)*100))\n",
    "#     print(\"----Finished stage 2----\\n\")\n",
    "#     if write:\n",
    "#         print('Writing results to \"{}\"'.format(out))\n",
    "#         merged_df.to_csv(out, index=False)\n",
    "\n",
    "#     return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd781f7f-7e54-42f3-8ff4-832ff67cfe61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def dedup_and_link(df, ep_df_path, out, dedup_settings_file, dedup_training_file, linking_settings_file, linking_training_file):\n",
    "#     # Start timer\n",
    "#     print(\"\\n/=========== Dedup x Record Linkage started ===========/\")\n",
    "#     start_time = time.time()\n",
    "#     # Phase 1: applying deduplication module to the data\n",
    "#     print(\"/=========== Start of phase 1: Deduplication ===========/\")\n",
    "#     deduped_data = deduplication(df, dedup_settings_file, dedup_training_file)\n",
    "\n",
    "#     # Phase 2: applying record linkage module to the data\n",
    "#     print(\"\\n\\n/=========== Start of phase 2: Record Linkage ===========/\")\n",
    "#     linked_data = record_linkage(deduped_data, pd.read_csv(ep_df_path), linking_settings_file, linking_training_file)\n",
    "#     end_time = time.time()\n",
    "\n",
    "#     print(\"\\n/=========== Dedup x Record Linkage finished. Duration: {} hours, {} minutes, {} seconds ===========/\".format(*seconds_conversion(end_time - start_time)))\n",
    "\n",
    "#     print('Writing results to \"{}\"'.format(out))\n",
    "#     linked_data.to_csv(out, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bce5a0a6-7e16-40d7-b16e-a892280c7d75",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4.3.1 Duplicate removal and Europages linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cd0ccff-d4b0-4894-934a-74f9f4705811",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Provided input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "135c7b1a-3ca4-4be9-bdfe-4afc5bdf07f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deduplication(translated_df, dedup_settings_file, dedup_training_file, write = True, out = deduped_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5aa97987-339a-4226-b824-8e020fd7b0a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Base Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "561a6c49-0d87-4e6f-ac1f-e0418cffb8eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# deduplication(base_data_file_location, dedup_settings_file, dedup_training_file, write = True, out = base_data_deduped_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97f8fda5-8e50-4e4d-a817-86be13a3ade1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### new Italy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40fcee2e-4267-4d2b-90a1-9a5c03858da5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# deduplication(italy_data_file_location, dedup_settings_file, dedup_training_file, write = True, out = italy_deduped_file)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4-DeduplicationXRecordLinkage",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
