{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ee8ce93-0fe4-4a1b-ac06-f903e54430bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 1. Typo correction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abb4f705-b911-413b-a295-e17013c23f47",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.1 Import the required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "922c33d3-4f90-4c1d-b25d-37955e860fc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU availability:False\n",
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, BooleanType\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.dbutils import DBUtils\n",
    "from pandarallel import pandarallel\n",
    "from transformers import pipeline\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import uuid\n",
    "\n",
    "# Define a namespace (this can be any UUID)\n",
    "namespace = uuid.NAMESPACE_DNS\n",
    "\n",
    "device = True if torch.cuda.is_available() else False\n",
    "print(\"GPU availability:{}\".format(device))\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "dbutils = DBUtils(spark) # important to have databricks cluster up and running. restart kernel to reflect cluster activity\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "# specify the languages to be used for language detection to keep it within the scope of possible languages\n",
    "languages = [Language.ENGLISH, Language.FRENCH, Language.GERMAN, Language.SPANISH, Language.ITALIAN, Language.DUTCH, Language.ARABIC, Language.CHINESE]\n",
    "language_detector = LanguageDetectorBuilder.from_languages(*languages).build()\n",
    "\n",
    "typo_corrector = pipeline(\"text2text-generation\", model=\"oliverguhr/spelling-correction-english-base\", max_length=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acfe9e88-a5c1-46f1-934e-f3075018fc49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.2 Load in the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "        StructField('company_name', StringType(), False),\n",
    "        StructField('group', StringType(), True),\n",
    "        StructField('sector', StringType(), True),\n",
    "        StructField('subsector', StringType(), True),\n",
    "        StructField('main_activity', StringType(), True),\n",
    "        StructField('address', StringType(), True),\n",
    "        StructField('company_city', StringType(), True),\n",
    "        StructField('postcode', StringType(), True),\n",
    "        StructField('country', StringType(), False),\n",
    "        StructField('products_and_services', StringType(), True),\n",
    "        StructField('information', StringType(), True),\n",
    "        StructField('min_headcount', StringType(), True),\n",
    "        StructField('max_headcount', StringType(), True),\n",
    "        StructField('type_of_building_for_registered_address', StringType(), True),\n",
    "        StructField('verified_by_europages', StringType(), True),\n",
    "        StructField('year_established', StringType(), True),\n",
    "        StructField('websites', StringType(), True),\n",
    "        StructField('download_datetime', StringType(), True),\n",
    "        StructField('id', StringType(), False),\n",
    "        StructField('filename', StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files():\n",
    "    if 'DATABRICKS_RUNTIME_VERSION' in os.environ:\n",
    "        files = [x[0] for x in dbutils.fs.ls(\"abfss://landingzone@storagetiltdevelop.dfs.core.windows.net/tiltEP/\")]\n",
    "    else:\n",
    "        files = [x[0] for x in dbutils.fs.ls(\"abfss:/mnt/indicatorBefore/tiltEP/\")]\n",
    "    # create a list in which the panda dataframes will be stored\n",
    "    df_list = []\n",
    "    for i in tqdm(range(len(files))):\n",
    "        # read every spark dataframe but only get the id and products_and_services column\n",
    "        df = spark.read.csv(files[i], header=True, schema=schema, sep=\";\").select(\"id\", \"products_and_services\").toPandas()\n",
    "        # add this dataframe to the list of dataframes\n",
    "        df_list.append(df) \n",
    "    # concatenate all the dataframes in the list into one dataframe\n",
    "    final_df = pd.concat(df_list)\n",
    "    return final_df\n",
    "\n",
    "def create_product_id( dataframe):\n",
    "    # Use uuid5 to generate a UUID based on the product name\n",
    "    dataframe['products_id'] = dataframe['products_and_services'].apply(lambda name: uuid.uuid5(namespace, name))\n",
    "    # extract company_id and products_id and export it\n",
    "    dataframe[[\"id\", \"products_id\"]].to_csv(\"../../data/example_data/output/tilt_company_products.csv\")\n",
    "    return dataframe\n",
    "\n",
    "def split_delimit(dataframe):\n",
    "    # create a copy of the dataframe\n",
    "    splitted_df = dataframe.copy()\n",
    "    # split the products_and_services column into a list of strings\n",
    "    splitted_df['products_and_services'] = splitted_df['products_and_services'].str.split('|')\n",
    "\n",
    "    # explode the products_and_services column into multiple rows\n",
    "    splitted_df = splitted_df.explode('products_and_services')  \n",
    "    # replace any empty strings with a null value\n",
    "    splitted_df[\"products_and_services\"] = splitted_df[\"products_and_services\"].replace(\"\", np.nan)\n",
    "\n",
    "    # drop any rows with a null value in the products_and_services column\n",
    "    splitted_df.dropna(subset=['products_and_services'], inplace=True)\n",
    "    splitted_df = splitted_df.reset_index(drop=True)\n",
    "\n",
    "    # remove whitespaces from products_and_services column\n",
    "    splitted_df[\"products_and_services\"] = splitted_df[\"products_and_services\"].str.strip()\n",
    "    splitted_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # return the dataframe\n",
    "    return splitted_df\n",
    "\n",
    "def explode_products():\n",
    "    return create_product_id(split_delimit(load_files()))[[\"products_id\", \"products_and_services\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1337/1337 [10:02<00:00,  2.22it/s] \n"
     ]
    }
   ],
   "source": [
    "x = explode_products()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44c9ee41-fffc-4ccb-b059-848cb88fe83f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d4878c8-3aed-4c67-aa74-081a3f7eab7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def conf_ld_detect_language(text):\n",
    "    \"\"\"Language detection wrapper.\n",
    "    \n",
    "    Returns detected language (ISO-code) and confidence of detection. In case of \n",
    "    failure of detection string 'ident_fail' and a pd.NA value for confidence is \n",
    "    returned.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The string for which language shall be detected.\n",
    "        model (str): The model to be used for language detection. Defaults to langdetect model.\n",
    "    Returns:\n",
    "        str: The detected language (ISO-code).\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        detected_language = language_detector.detect_language_of(text).iso_code_639_1.name.lower()\n",
    "        return detected_language\n",
    "    except:   \n",
    "        return \"ident_fail\", pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b67247a-081d-4b07-8b77-f93269d1a983",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def typo_correction(text=\"\", model=\"default\"):\n",
    "    \"\"\"Typo correction wrapper.\n",
    "    \n",
    "    Returns corrected text. In case of failure of correction the original text \n",
    "    is returned. \n",
    "    \n",
    "    Args:\n",
    "        text (str): The string to be corrected.\n",
    "        model (str): The model to be used for typo correction. Defaults to textblob model.\n",
    "    Returns:\n",
    "        str: The corrected string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if model == \"default\":\n",
    "            return(TextBlob(text).correct().string)\n",
    "        elif model == \"huggingface\":\n",
    "            return(typo_corrector(text)[0][\"generated_text\"])\n",
    "    except:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5362807-db94-4297-ad28-2d571b5e1504",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Typo correction module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1becd22c-5622-4dbe-af8e-d3c2ca4b802d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def typo_correct_df(df):\n",
    "    \"\"\"Typo correction wrapper for dataframes.\n",
    "    \n",
    "    Returns dataframe with corrected text. In case of failure of correction the \n",
    "    original text is returned. \n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe containing the text to be corrected.\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe with corrected text.\n",
    "    \"\"\"\n",
    "    # detect the language of the text but only for the rows that do not have a value in the automatic_processed_products_and_services column\n",
    "    print(\"Detecting the language of the text...\")\n",
    "    # only take rows that have a True value in the to_process column\n",
    "    to_process_df = df.copy()\n",
    "    to_process_df.loc[:, \"language (ISO-code)\"] = to_process_df[\"products_and_services\"].parallel_apply(conf_ld_detect_language)\n",
    "\n",
    "    # then take subset of english texts\n",
    "    print(\"Taking subset of English texts...\")\n",
    "    english_df = to_process_df[to_process_df[\"language (ISO-code)\"] == \"en\"]\n",
    "    # exclude enlgish texts from the original df\n",
    "    to_process_df = to_process_df[to_process_df[\"language (ISO-code)\"] != \"en\"]\n",
    "\n",
    "    # apply typo correction to english texts\n",
    "    print(\"Applying typo correction...\")\n",
    "    english_df = english_df.copy()\n",
    "    english_df.loc[:, \"typo_corrected\"] = english_df[\"products_and_services\"].parallel_apply(typo_correction)\n",
    "\n",
    "    # merge the corrected english texts with the original df\n",
    "    print(\"Merging the corrected english texts with the original df...\")\n",
    "    df = pd.concat([to_process_df, english_df], ignore_index=True)\n",
    "    # replace empty values in typo_corrected with the original text\n",
    "    df[\"typo_corrected\"].fillna(df[\"products_and_services\"], inplace=True)\n",
    "    # make typo_corrected lowercase and remove all dots at the end\n",
    "    df[\"typo_corrected\"] = df[\"typo_corrected\"].str.lower().str.replace(\"\\.$\", \"\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3b616ba-26dc-4e2e-94a0-78f012688150",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Provided input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bac8dd0a-9fc0-4110-a979-ade640232982",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting the language of the text...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb1294dd7dc4ef48740b0875f09e5d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=5453), Label(value='0 / 5453'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking subset of English texts...\n",
      "Applying typo correction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76e31a697a84fa7ad39d285088bbbe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=4583), Label(value='0 / 4583'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging the corrected english texts with the original df...\n"
     ]
    }
   ],
   "source": [
    "corrected_df = typo_correct_df(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdc5767d-ddf9-4d7b-95e5-acf9a0100269",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.4 Export the dataframe with the corrected text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7e1b6d1-4480-415b-8956-297a956d74da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if 'DATABRICKS_RUNTIME_VERSION' in os.environ:\n",
    "    pass\n",
    "else:\n",
    "    # Define the path for the new dataframe\n",
    "    output_path_typo_corrected = \"../../data/example_data/output/tilt_products_typo_corrected.csv\"\n",
    "    # Write the new dataframe to the path\n",
    "    corrected_df.to_csv(output_path_typo_corrected, index=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2-Typo_correction",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
