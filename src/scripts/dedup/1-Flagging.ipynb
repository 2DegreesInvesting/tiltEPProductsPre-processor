{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44a07a79-012d-41fe-84f6-b8d2f4044390",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 1. Filtering\n",
    "Before we are going to clean up the data and apply pre-processing steps, we will first see if there are not already products and services which are already directly matching with the EP catalogue (ASSUMING THAT THE EP CATALOGUS CONTAINS CLEAN DATA). This reduces the number of records we would need to pre-process. Since we still need those records for the final output, we will flag them by adding a link to the EP catalogue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6643c915-6967-419c-8de8-c05a5daedeb4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.1 Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "922aef30-58bb-4ddd-bcc3-0e8852310be1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16fec71d-e6fa-4625-8aab-61cb21dd1516",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.2 Load in the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "665f1cac-3224-427b-803a-445df67ca747",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path = \"abfss://preprocessing@storagetiltdevelop.dfs.core.windows.net/data/example_data/input/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "585fc632-0b2d-4020-a46a-38f5ed9a1a71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if 'DATABRICKS_RUNTIME_VERSION' in os.environ:\n",
    "    def file_exists(path):\n",
    "        try:\n",
    "            dbutils.fs.ls(path)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            if 'java.io.FileNotFoundException' in str(e):\n",
    "                raise FileNotFoundError(\"File could not be found. Are you sure the file exists in the provided directory?\")\n",
    "            else:\n",
    "                raise\n",
    "    # Determine the location of the dataframe containing the company based products and services\n",
    "    europages_activities_catalogue_location = \"abfss://preprocessing@storagetiltdevelop.dfs.core.windows.net/data/example_data/input/scraped_data/scraped_EP_products_catalogue.csv\"\n",
    "    file_path = dbutils.widgets.get(\"FilePath\")\n",
    "    file_path = str(os.path.join(base_path, file_path).replace(\"\\\\\", \"/\"))\n",
    "    \n",
    "    # Read the dataframe\n",
    "    europages_activities_catalogue = spark.read.option(\"header\",True).csv(europages_activities_catalogue_location).toPandas()\n",
    "    # check if file exists\n",
    "    file_exists(file_path)\n",
    "    file_df = spark.read.option(\"header\",True).csv(file_path).toPandas()\n",
    "\n",
    "else:\n",
    "    # Determine the location of the dataframe EuroPages catalogue\n",
    "    europages_activities_catalogue_location = \"../../data/example_data/input/scraped_data/scraped_EP_products_catalogue.csv\"\n",
    "    # Determine the location of the dataframe containing unprocessed products and services\n",
    "    base_tilt_data_location = \"../../data/example_data/input/tilt_base_products_and_services_unprocessed.csv\"\n",
    "    # Determine the location of the dataframe containing the unprocessed products and services\n",
    "    italy_tilt_data_location = \"../../data/example_data/input/tilt_italy_products_and_services_unprocessed.csv\"\n",
    "    \n",
    "    # Read the dataframe\n",
    "    europages_activities_catalogue  = pd.read_csv(europages_activities_catalogue_location)\n",
    "    # Read the base_data\n",
    "    base_tilt_data = pd.read_csv(base_tilt_data_location)\n",
    "    # Read the italy data\n",
    "    italy_tilt_data = pd.read_csv(italy_tilt_data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "332b624f-4894-4073-8de0-7599b245c81e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.3 Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31c3daaa-a4e0-4323-b98d-d7e545950146",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def exclude_covered_records(df_1, df_2):\n",
    "    # Merge the two dataframes based on the 'products_and_services' column\n",
    "    merged_df = df_1.merge(df_2, on='products_and_services', how='inner')\n",
    "\n",
    "    # Remove ID column and rename products_id_y to linke_EP_products_id\n",
    "    merged_df = merged_df.drop(['ID', 'products_id_y'], axis=1)\n",
    "    merged_df = merged_df.rename(columns={'products_id_x': 'products_id'})\n",
    "    \n",
    "    # add column called to_process which labels the records that need to be processed\n",
    "    merged_df['to_process'] = False\n",
    "    \n",
    "\n",
    "    # concatentate the merged_df with the records in df_1 that arent in merged_df based of products_id\n",
    "    df_1 = df_1[~df_1['products_id'].isin(merged_df['products_id'])]\n",
    "    merged_df = pd.concat([merged_df, df_1], ignore_index=True)\t\n",
    "    # replace the NaN values in the to_process column with \"yes\"\n",
    "    merged_df['to_process'] = merged_df['to_process'].fillna(True)\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d5cadce-6771-40a6-8b59-df1c1e206ca5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Provided input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "715d62df-e63d-45e5-97ac-543f47bb45b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filtered_file_df = exclude_covered_records(file_df, europages_activities_catalogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0daa1bc-8392-4e5e-aaab-46a8924d84e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Base Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed8a3f81-3796-48fd-b34e-31469bb63e95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filtered_base_data = exclude_covered_records(base_tilt_data, europages_activities_catalogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cb7e189-8a4c-4400-937c-2b5fd035eebf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### tilt Italy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72fdb25c-4a96-4a7b-999f-f0101f514bda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filtered_italy_data = exclude_covered_records(italy_tilt_data, europages_activities_catalogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "681cadb4-ac53-480e-944c-bd0db456d6f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.4 Export the filtered dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0593a23-fdf3-41dd-8aa1-e5759632ef54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if 'DATABRICKS_RUNTIME_VERSION' in os.environ:\n",
    "    # Define the path for the new dataframe\n",
    "    output_path = str(os.path.join(os.path.dirname(os.path.dirname(file_path)), \"output/flagged_\" + os.path.basename(file_path))).replace(\"\\\\\", \"/\")    \n",
    "    # Convert the pandas dataframe to a spark sql dataframe\n",
    "    filtered_df_spark = spark.createDataFrame(filtered_file_df)\n",
    "    # Write the new dataframe to the path\n",
    "    filtered_df_spark.write.csv(output_path, mode=\"overwrite\", header=True)\n",
    "    dbutils.jobs.taskValues.set(key = 'OutPath', value = output_path)\n",
    "else:\n",
    "    # Define the path for the new dataframe\n",
    "    output_filtered_base_data = \"../../data/example_data/output/base_data/base_flagged_products.csv\"\n",
    "    # Define the path for the new dataframe\n",
    "    output_filtered_italy_data = \"../../data/example_data/output/italy_data/italy_flagged_products.csv\"\n",
    "\n",
    "    # Write the new dataframe to the path\n",
    "    filtered_base_data.to_csv(output_filtered_base_data,index = False)\n",
    "    # Write the new dataframe to the path\n",
    "    filtered_italy_data.to_csv(output_filtered_italy_data,index = False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1-Flagging",
   "widgets": {
    "file_path": {
     "currentValue": "",
     "nuid": "0802d3ce-748d-4a4f-84b6-a6483ffc64b2",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "NONE",
      "label": null,
      "name": "file_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
