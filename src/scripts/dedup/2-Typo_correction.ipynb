{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ee8ce93-0fe4-4a1b-ac06-f903e54430bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2. Typo correction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abb4f705-b911-413b-a295-e17013c23f47",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2.1 Import the required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "922c33d3-4f90-4c1d-b25d-37955e860fc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from langdetect import detect_langs, DetectorFactory\n",
    "from transformers import pipeline\n",
    "from textblob import TextBlob\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "device = True if torch.cuda.is_available() else False\n",
    "print(\"GPU availability:{}\".format(device))\n",
    "\n",
    "language_detector = pipeline(\"text-classification\", model=\"papluca/xlm-roberta-base-language-detection\") # this model is 1.1 gigabyte so it will take around 5 mins to download it\n",
    "typo_corrector = pipeline(\"text2text-generation\", model=\"oliverguhr/spelling-correction-english-base\", max_length=1000)\n",
    "\n",
    "DetectorFactory.seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acfe9e88-a5c1-46f1-934e-f3075018fc49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2.2 Load in the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d3a5cf7-8184-4659-8799-18ac5e24cd85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if 'DATABRICKS_RUNTIME_VERSION' in os.environ:\n",
    "    def file_exists(path):\n",
    "        try:\n",
    "            dbutils.fs.ls(path)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            if 'java.io.FileNotFoundException' in str(e):\n",
    "                raise FileNotFoundError(\"File could not be found. Are you sure the file exists in the provided directory?\")\n",
    "            else:\n",
    "                raise\n",
    "    input_path = dbutils.jobs.taskValues.get(taskKey = \"Flagging\", key = \"OutPath\", default = \"None\", debugValue = 0)\n",
    "    file_exists(input_path)\n",
    "    flagged_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\",\"true\").csv(input_path).toPandas()\n",
    "else:\n",
    "    # Determine the location of the dataframe containing the translated text\n",
    "    base_tilt_data_location = \"../../data/example_data/output/base_data/base_flagged_products.csv\"\n",
    "    # Determine the location of the dataframe containing the translated text\n",
    "    italy_tilt_data_location = \"../../data/example_data/output/italy_data/italy_flagged_products.csv\"\n",
    "    \n",
    "    # use raw_df\n",
    "    base_tilt_data = pd.read_csv(base_tilt_data_location)\n",
    "    # use raw_df\n",
    "    italy_tilt_data = pd.read_csv(italy_tilt_data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a5879f9-2d10-4202-abd2-a1170e67cb5e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2.3 Apply typo correction module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44c9ee41-fffc-4ccb-b059-848cb88fe83f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d4878c8-3aed-4c67-aa74-081a3f7eab7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def conf_ld_detect_language(text, model=\"def\"):\n",
    "    \"\"\"Language detection wrapper.\n",
    "    \n",
    "    Returns detected language (ISO-code) and confidence of detection. In case of \n",
    "    failure of detection string 'ident_fail' and a pd.NA value for confidence is \n",
    "    returned.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The string for which language shall be detected.\n",
    "        model (str): The model to be used for language detection. Defaults to langdetect model.\n",
    "    Returns:\n",
    "        str: The detected language (ISO-code).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if model == \"def\":\n",
    "            highest_conf = detect_langs(text)[0]\n",
    "            return highest_conf.lang\n",
    "        elif model == \"huggingface\":\n",
    "            result = language_detector(text)[0]\n",
    "            return str(result[\"label\"])\n",
    "    except:   \n",
    "        return \"ident_fail\", pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b67247a-081d-4b07-8b77-f93269d1a983",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def typo_correction(text=\"\", model=\"def\"):\n",
    "    \"\"\"Typo correction wrapper.\n",
    "    \n",
    "    Returns corrected text. In case of failure of correction the original text \n",
    "    is returned. \n",
    "    \n",
    "    Args:\n",
    "        text (str): The string to be corrected.\n",
    "        model (str): The model to be used for typo correction. Defaults to textblob model.\n",
    "    Returns:\n",
    "        str: The corrected string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if model == \"def\":\n",
    "            return(TextBlob(text).correct().string)\n",
    "        elif model == \"huggingface\":\n",
    "            return(typo_corrector(text)[0][\"generated_text\"])\n",
    "    except:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5362807-db94-4297-ad28-2d571b5e1504",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Typo correction module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1becd22c-5622-4dbe-af8e-d3c2ca4b802d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def typo_correct_df(df):\n",
    "    \"\"\"Typo correction wrapper for dataframes.\n",
    "    \n",
    "    Returns dataframe with corrected text. In case of failure of correction the \n",
    "    original text is returned. \n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe containing the text to be corrected.\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe with corrected text.\n",
    "    \"\"\"\n",
    "    # detect the language of the text but only for the rows that do not have a value in the automatic_processed_products_and_services column\n",
    "    print(\"Detecting the language of the text...\")\n",
    "    # only take rows that have a True value in the to_process column\n",
    "    to_process_df = df[df[\"to_process\"] == True].copy()\n",
    "    # exclude to_processed_df rows from df\n",
    "    df = df[df[\"to_process\"] == False].copy()\n",
    "    to_process_df.loc[:, \"language (ISO-code)\"] = to_process_df[\"products_and_services\"].progress_apply(lambda x: conf_ld_detect_language(x, model=\"huggingface\"))\n",
    "\n",
    "    # then take subset of english texts\n",
    "    print(\"Taking subset of English texts...\")\n",
    "    english_df = to_process_df[to_process_df[\"language (ISO-code)\"] == \"en\"]\n",
    "    # exclude enlgish texts from the original df\n",
    "    to_process_df = to_process_df[to_process_df[\"language (ISO-code)\"] != \"en\"]\n",
    "\n",
    "    # apply typo correction to english texts\n",
    "    print(\"Applying typo correction...\")\n",
    "    english_df = english_df.copy()\n",
    "    english_df.loc[:, \"typo_corrected\"] = english_df[\"products_and_services\"].progress_apply(lambda x: typo_correction(x, model=\"huggingface\"))\n",
    "\n",
    "    # merge the corrected english texts with the original df\n",
    "    print(\"Merging the corrected english texts with the original df...\")\n",
    "    df = pd.concat([to_process_df, english_df, df], ignore_index=True)\n",
    "    # replace empty values in typo_corrected with the original text\n",
    "    df[\"typo_corrected\"].fillna(df[\"products_and_services\"], inplace=True)\n",
    "    # make typo_corrected lowercase and remove all dots at the end\n",
    "    df[\"typo_corrected\"] = df[\"typo_corrected\"].str.lower().str.replace(\"\\.$\", \"\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3b616ba-26dc-4e2e-94a0-78f012688150",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Provided input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bac8dd0a-9fc0-4110-a979-ade640232982",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "corrected_df = typo_correct_df(flagged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ddda3a0-853f-49e7-ae2a-02578cec5a0f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Base Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb280080-49b5-4e98-8c61-3e7c7c213af8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# base_typo_corrected_df = typo_correct_df(base_tilt_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61141dc9-8336-4779-a61d-3493a520333a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### tilt Italy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a838aa7-66d7-4b66-a553-4f54e64aae57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# italy_typo_corrected_df = typo_correct_df(italy_tilt_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdc5767d-ddf9-4d7b-95e5-acf9a0100269",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2.4 Export the dataframe with the corrected text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7e1b6d1-4480-415b-8956-297a956d74da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if 'DATABRICKS_RUNTIME_VERSION' in os.environ:\n",
    "    output_path = str(os.path.join(os.path.dirname(os.path.dirname(input_path)), \"output/corrected_\" + os.path.basename(input_path))).replace(\"\\\\\", \"/\")    \n",
    "    # Convert the pandas dataframe to a spark sql dataframe\n",
    "    corrected_df_spark = spark.createDataFrame(corrected_df)\n",
    "    # Write the new dataframe to the path\n",
    "    corrected_df_spark.write.csv(output_path, mode=\"overwrite\", header=True)\n",
    "    dbutils.jobs.taskValues.set(key = 'OutPath', value = output_path)\n",
    "    \n",
    "else:\n",
    "    # Define the path for the new dataframe\n",
    "    output_path_base_typo_corrected = \"../../data/example_data/output/base_data/base_typo_corrected_products.csv\"\n",
    "    # Define the path for the new dataframe\n",
    "    output_path_italy_typo_corrected = \"../../data/example_data/output/italy_data/italy_typo_corrected_products.csv\"\n",
    "\n",
    "    # Write the new dataframe to the path\n",
    "    base_typo_corrected_df.to_csv(output_path_base_typo_corrected, index=False)\n",
    "    # Write the new dataframe to the path\n",
    "    italy_typo_corrected_df.to_csv(output_path_italy_typo_corrected, index=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2-Typo_correction",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
