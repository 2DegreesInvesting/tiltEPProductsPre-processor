{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\selsabrouty\\AppData\\Local\\anaconda3\\envs\\new-tilt-nlp-preprocessing\\lib\\site-packages\\umap\\distances.py:1053: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\selsabrouty\\AppData\\Local\\anaconda3\\envs\\new-tilt-nlp-preprocessing\\lib\\site-packages\\umap\\distances.py:1061: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\selsabrouty\\AppData\\Local\\anaconda3\\envs\\new-tilt-nlp-preprocessing\\lib\\site-packages\\umap\\distances.py:1076: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\selsabrouty\\AppData\\Local\\anaconda3\\envs\\new-tilt-nlp-preprocessing\\lib\\site-packages\\umap\\umap_.py:646: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\selsabrouty\\AppData\\Local\\anaconda3\\envs\\new-tilt-nlp-preprocessing\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\selsabrouty\\AppData\\Local\\anaconda3\\envs\\new-tilt-nlp-preprocessing\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\util.py:74: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\selsabrouty\\AppData\\Local\\anaconda3\\envs\\new-tilt-nlp-preprocessing\\lib\\site-packages\\tensorflow_hub\\native_module.py:92: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\selsabrouty\\AppData\\Local\\anaconda3\\envs\\new-tilt-nlp-preprocessing\\lib\\site-packages\\tensorflow_hub\\saved_model_module.py:40: The name tf.saved_model.constants.LEGACY_INIT_OP_KEY is deprecated. Please use tf.compat.v1.saved_model.constants.LEGACY_INIT_OP_KEY instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import hdbscan\n",
    "import umap.umap_ as umap\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from tqdm.notebook import trange\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, space_eval, Trials\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 600)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"max_colwidth\", 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>products_id</th>\n",
       "      <th>products_and_services</th>\n",
       "      <th>clustered_id</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>164399edbf8e880dc2e856f50d51e720bd0a8abe</td>\n",
       "      <td>fish, frozen and deep-frozen</td>\n",
       "      <td>a18df3877d3f9598d7c8fbae0adc2cad4acf37c6</td>\n",
       "      <td>fish frozen deepfrozen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b0d3c55743b1b858ec2843c8870116bb8af543fd</td>\n",
       "      <td>drilling and test boring - equipment</td>\n",
       "      <td>49659f8efe8d9a92455f0d378783469558ae7df1</td>\n",
       "      <td>drilling test boring equipment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b14c038972e6a52bfbf3ffbe77def57a62c5b9cf</td>\n",
       "      <td>well-management services</td>\n",
       "      <td>b14c038972e6a52bfbf3ffbe77def57a62c5b9cf</td>\n",
       "      <td>wellmanagement service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abadc2542b4b5c1ecfe41c22afb2347b1d9b65af</td>\n",
       "      <td>electronic data processing - software</td>\n",
       "      <td>35596a3df5495e2dc5d18cff45c58cadda91040c</td>\n",
       "      <td>electronic data processing software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60c58ad2ef34d96fae028f1039fab03dec9eb9a2</td>\n",
       "      <td>communication</td>\n",
       "      <td>60c58ad2ef34d96fae028f1039fab03dec9eb9a2</td>\n",
       "      <td>communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31541</th>\n",
       "      <td>a56bfdd9971ddba76de33e5dd394faab63d2c58c</td>\n",
       "      <td>trading in non-ferrous products</td>\n",
       "      <td>5af4a5f264253d48a9504c6e9e9de651f5528121</td>\n",
       "      <td>trading nonferrous product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31542</th>\n",
       "      <td>d16685f9db86a7e446d5a4c763a17016ffdfa613</td>\n",
       "      <td>precision weights for scales</td>\n",
       "      <td>b52520ccdfafa1b05949ffe08c0fdde9e2556a9e</td>\n",
       "      <td>precision weight scale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31543</th>\n",
       "      <td>37c8e6d302d907a76f49d45a91949c86dd5fcc03</td>\n",
       "      <td>weights and masses - measurement and verification instruments</td>\n",
       "      <td>822c0e12996351ae9cf05354936d074bb4c6103b</td>\n",
       "      <td>weight mass measurement verification instrument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31544</th>\n",
       "      <td>4aa756effa61af41058cf80f475a03b439232cfe</td>\n",
       "      <td>manicure scissors</td>\n",
       "      <td>4aa756effa61af41058cf80f475a03b439232cfe</td>\n",
       "      <td>manicure scissors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31545</th>\n",
       "      <td>c732592a40981d7bd20a41585672204b41f93b12</td>\n",
       "      <td>mozzarella for pizza</td>\n",
       "      <td>c732592a40981d7bd20a41585672204b41f93b12</td>\n",
       "      <td>mozzarella pizza</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31546 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    products_id  \\\n",
       "0      164399edbf8e880dc2e856f50d51e720bd0a8abe   \n",
       "1      b0d3c55743b1b858ec2843c8870116bb8af543fd   \n",
       "2      b14c038972e6a52bfbf3ffbe77def57a62c5b9cf   \n",
       "3      abadc2542b4b5c1ecfe41c22afb2347b1d9b65af   \n",
       "4      60c58ad2ef34d96fae028f1039fab03dec9eb9a2   \n",
       "...                                         ...   \n",
       "31541  a56bfdd9971ddba76de33e5dd394faab63d2c58c   \n",
       "31542  d16685f9db86a7e446d5a4c763a17016ffdfa613   \n",
       "31543  37c8e6d302d907a76f49d45a91949c86dd5fcc03   \n",
       "31544  4aa756effa61af41058cf80f475a03b439232cfe   \n",
       "31545  c732592a40981d7bd20a41585672204b41f93b12   \n",
       "\n",
       "                                               products_and_services  \\\n",
       "0                                       fish, frozen and deep-frozen   \n",
       "1                               drilling and test boring - equipment   \n",
       "2                                           well-management services   \n",
       "3                              electronic data processing - software   \n",
       "4                                                      communication   \n",
       "...                                                              ...   \n",
       "31541                                trading in non-ferrous products   \n",
       "31542                                   precision weights for scales   \n",
       "31543  weights and masses - measurement and verification instruments   \n",
       "31544                                              manicure scissors   \n",
       "31545                                           mozzarella for pizza   \n",
       "\n",
       "                                   clustered_id  \\\n",
       "0      a18df3877d3f9598d7c8fbae0adc2cad4acf37c6   \n",
       "1      49659f8efe8d9a92455f0d378783469558ae7df1   \n",
       "2      b14c038972e6a52bfbf3ffbe77def57a62c5b9cf   \n",
       "3      35596a3df5495e2dc5d18cff45c58cadda91040c   \n",
       "4      60c58ad2ef34d96fae028f1039fab03dec9eb9a2   \n",
       "...                                         ...   \n",
       "31541  5af4a5f264253d48a9504c6e9e9de651f5528121   \n",
       "31542  b52520ccdfafa1b05949ffe08c0fdde9e2556a9e   \n",
       "31543  822c0e12996351ae9cf05354936d074bb4c6103b   \n",
       "31544  4aa756effa61af41058cf80f475a03b439232cfe   \n",
       "31545  c732592a40981d7bd20a41585672204b41f93b12   \n",
       "\n",
       "                                          cleaned_text  \n",
       "0                               fish frozen deepfrozen  \n",
       "1                       drilling test boring equipment  \n",
       "2                               wellmanagement service  \n",
       "3                  electronic data processing software  \n",
       "4                                        communication  \n",
       "...                                                ...  \n",
       "31541                       trading nonferrous product  \n",
       "31542                           precision weight scale  \n",
       "31543  weight mass measurement verification instrument  \n",
       "31544                                manicure scissors  \n",
       "31545                                 mozzarella pizza  \n",
       "\n",
       "[31546 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Determine the location of the dataframe containing the typo-corrected text\n",
    "file_location = \"../data/example_data/output/cleaned_products.parquet\"\n",
    "\n",
    "# Read the dataframe\n",
    "full_df = pd.read_parquet(file_location)\n",
    "\n",
    "# Display dataframe\n",
    "display(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sentences = list(full_df[\"cleaned_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\selsabrouty\\AppData\\Local\\anaconda3\\envs\\new-tilt-nlp-preprocessing\\lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\selsabrouty\\AppData\\Local\\anaconda3\\envs\\new-tilt-nlp-preprocessing\\lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\selsabrouty\\AppData\\Local\\anaconda3\\envs\\new-tilt-nlp-preprocessing\\lib\\site-packages\\tensorflow_hub\\module_v2.py:120: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\selsabrouty\\AppData\\Local\\anaconda3\\envs\\new-tilt-nlp-preprocessing\\lib\\site-packages\\tensorflow_hub\\module_v2.py:120: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
     ]
    }
   ],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "model_use = hub.load(module_url)\n",
    "\n",
    "print(f\"module {module_url} loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_st1 = SentenceTransformer('all-mpnet-base-v2')\n",
    "model_st2 = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model_st3 = SentenceTransformer('all-distilroberta-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(model, model_type, sentences):\n",
    "    if model_type == 'use':\n",
    "        embeddings = model(sentences)\n",
    "    elif model_type == 'sentence transformer':\n",
    "        embeddings = model.encode(sentences, show_progress_bar=True, batch_size=128)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7871d8b87140419bfdf64fb879a70d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd4138549754a018d0466e5f91c5f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ebe6961282342fb8cca2c8c1554590c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_use = embed(model_use, 'use', corpus_sentences)\n",
    "embeddings_st1 = embed(model_st1, 'sentence transformer', corpus_sentences)\n",
    "embeddings_st2 = embed(model_st2, 'sentence transformer', corpus_sentences)\n",
    "embeddings_st3 = embed(model_st3, 'sentence transformer', corpus_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31546, 512)\n",
      "(31546, 768)\n",
      "(31546, 384)\n",
      "(31546, 768)\n"
     ]
    }
   ],
   "source": [
    "embeddings = [embeddings_use, embeddings_st1, embeddings_st2, embeddings_st3]\n",
    "\n",
    "for embedding in embeddings:\n",
    "    print(embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clusters(message_embeddings,\n",
    "                      n_neighbors,\n",
    "                      n_components, \n",
    "                      min_cluster_size,\n",
    "                      min_samples = None,\n",
    "                      random_state = None):\n",
    "    \"\"\"\n",
    "    Returns HDBSCAN objects after first performing dimensionality reduction using UMAP\n",
    "    \n",
    "    Arguments:\n",
    "        message_embeddings: embeddings to use\n",
    "        n_neighbors: int, UMAP hyperparameter n_neighbors\n",
    "        n_components: int, UMAP hyperparameter n_components\n",
    "        min_cluster_size: int, HDBSCAN hyperparameter min_cluster_size\n",
    "        min_samples: int, HDBSCAN hyperparameter min_samples\n",
    "        random_state: int, random seed\n",
    "        \n",
    "    Returns:\n",
    "        clusters: HDBSCAN object of clusters\n",
    "    \"\"\"\n",
    "    \n",
    "    umap_embeddings = (umap.UMAP(n_neighbors = n_neighbors, \n",
    "                                n_components = n_components, \n",
    "                                metric = 'cosine', \n",
    "                                random_state=random_state)\n",
    "                            .fit_transform(message_embeddings))\n",
    "\n",
    "    clusters = hdbscan.HDBSCAN(min_cluster_size = min_cluster_size, \n",
    "                               min_samples = min_samples,\n",
    "                               metric='euclidean', \n",
    "                               gen_min_span_tree=True,\n",
    "                               cluster_selection_method='eom').fit(umap_embeddings)\n",
    "    \n",
    "    return clusters\n",
    "def score_clusters(clusters, prob_threshold = 0.05):\n",
    "    \"\"\"\n",
    "    Returns the label count and cost of a given clustering\n",
    "\n",
    "    Arguments:\n",
    "        clusters: HDBSCAN clustering object\n",
    "        prob_threshold: float, probability threshold to use for deciding\n",
    "                        what cluster labels are considered low confidence\n",
    "\n",
    "    Returns:\n",
    "        label_count: int, number of unique cluster labels, including noise\n",
    "        cost: float, fraction of data points whose cluster assignment has\n",
    "              a probability below cutoff threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    cluster_labels = clusters.labels_\n",
    "    label_count = len(np.unique(cluster_labels))\n",
    "    total_num = len(clusters.labels_)\n",
    "    cost = (np.count_nonzero(clusters.probabilities_ < prob_threshold)/total_num)\n",
    "    \n",
    "    return label_count, cost\n",
    "def random_search(embeddings, space, num_evals):\n",
    "    \"\"\"\n",
    "    Randomly search parameter space of clustering pipeline\n",
    "\n",
    "    Arguments:\n",
    "        embeddings: embeddings to use\n",
    "        space: dict, contains keys for 'n_neighbors', 'n_components',\n",
    "               and 'min_cluster_size' and values with\n",
    "               corresponding lists or ranges of parameters to search\n",
    "        num_evals: int, number of random parameter combinations to try\n",
    "\n",
    "    Returns:\n",
    "        df_result: pandas dataframe containing info on each evaluation\n",
    "                   performed, including run_id, parameters used, label\n",
    "                   count, and cost\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in trange(num_evals):\n",
    "        n_neighbors = random.choice(space['n_neighbors'])\n",
    "        n_components = random.choice(space['n_components'])\n",
    "        min_cluster_size = random.choice(space['min_cluster_size'])\n",
    "        random_state = space['random_state']\n",
    "        \n",
    "        clusters = generate_clusters(embeddings, \n",
    "                                     n_neighbors=n_neighbors, \n",
    "                                     n_components=n_components, \n",
    "                                     min_cluster_size=min_cluster_size, \n",
    "                                     random_state=random_state)\n",
    "    \n",
    "        label_count, cost = score_clusters(clusters, prob_threshold = 0.05)\n",
    "                \n",
    "        results.append([i, n_neighbors, n_components, min_cluster_size, label_count, cost])\n",
    "    \n",
    "    result_df = pd.DataFrame(results, columns=['run_id', 'n_neighbors', 'n_components', \n",
    "                                               'min_cluster_size', 'label_count', 'cost'])\n",
    "    \n",
    "    return result_df.sort_values(by='cost')\n",
    "def objective(params, embeddings, label_lower, label_upper):\n",
    "    \"\"\"\n",
    "    Objective function for hyperopt to minimize\n",
    "\n",
    "    Arguments:\n",
    "        params: dict, contains keys for 'n_neighbors', 'n_components',\n",
    "               'min_cluster_size', 'random_state' and\n",
    "               their values to use for evaluation\n",
    "        embeddings: embeddings to use\n",
    "        label_lower: int, lower end of range of number of expected clusters\n",
    "        label_upper: int, upper end of range of number of expected clusters\n",
    "\n",
    "    Returns:\n",
    "        loss: cost function result incorporating penalties for falling\n",
    "              outside desired range for number of clusters\n",
    "        label_count: int, number of unique cluster labels, including noise\n",
    "        status: string, hypoeropt status\n",
    "\n",
    "        \"\"\"\n",
    "    \n",
    "    clusters = generate_clusters(embeddings, \n",
    "                                 n_neighbors = params['n_neighbors'], \n",
    "                                 n_components = params['n_components'], \n",
    "                                 min_cluster_size = params['min_cluster_size'],\n",
    "                                 random_state = params['random_state'])\n",
    "    \n",
    "    label_count, cost = score_clusters(clusters, prob_threshold = 0.05)\n",
    "    \n",
    "    #15% penalty on the cost function if outside the desired range of groups\n",
    "    if (label_count < label_lower) | (label_count > label_upper):\n",
    "        penalty = 0.15 \n",
    "    else:\n",
    "        penalty = 0\n",
    "    \n",
    "    loss = cost + penalty\n",
    "    \n",
    "    return {'loss': loss, 'label_count': label_count, 'status': STATUS_OK}\n",
    "def bayesian_search(embeddings, space, label_lower, label_upper, max_evals=100):\n",
    "    \"\"\"\n",
    "    Perform bayesian search on hyperparameter space using hyperopt\n",
    "\n",
    "    Arguments:\n",
    "        embeddings: embeddings to use\n",
    "        space: dict, contains keys for 'n_neighbors', 'n_components',\n",
    "               'min_cluster_size', and 'random_state' and\n",
    "               values that use built-in hyperopt functions to define\n",
    "               search spaces for each\n",
    "        label_lower: int, lower end of range of number of expected clusters\n",
    "        label_upper: int, upper end of range of number of expected clusters\n",
    "        max_evals: int, maximum number of parameter combinations to try\n",
    "\n",
    "    Saves the following to instance variables:\n",
    "        best_params: dict, contains keys for 'n_neighbors', 'n_components',\n",
    "               'min_cluster_size', 'min_samples', and 'random_state' and\n",
    "               values associated with lowest cost scenario tested\n",
    "        best_clusters: HDBSCAN object associated with lowest cost scenario\n",
    "                       tested\n",
    "        trials: hyperopt trials object for search\n",
    "\n",
    "        \"\"\"\n",
    "    \n",
    "    trials = Trials()\n",
    "    fmin_objective = partial(objective, \n",
    "                             embeddings=embeddings, \n",
    "                             label_lower=label_lower,\n",
    "                             label_upper=label_upper)\n",
    "    \n",
    "    best = fmin(fmin_objective, \n",
    "                space = space, \n",
    "                algo=tpe.suggest,\n",
    "                max_evals=max_evals, \n",
    "                trials=trials)\n",
    "\n",
    "    best_params = space_eval(space, best)\n",
    "    print ('best:')\n",
    "    print (best_params)\n",
    "    print (f\"label count: {trials.best_trial['result']['label_count']}\")\n",
    "    \n",
    "    best_clusters = generate_clusters(embeddings, \n",
    "                                      n_neighbors = best_params['n_neighbors'], \n",
    "                                      n_components = best_params['n_components'], \n",
    "                                      min_cluster_size = best_params['min_cluster_size'],\n",
    "                                      random_state = best_params['random_state'])\n",
    "    \n",
    "    return best_params, best_clusters, trials\n",
    "def combine_results(df_ground, cluster_dict):\n",
    "    \"\"\"\n",
    "    Returns dataframe of all documents and each model's assigned cluster\n",
    "\n",
    "    Arguments:\n",
    "        df_ground: dataframe of original documents with associated ground truth\n",
    "                   labels\n",
    "        cluster_dict: dict, keys as column name for specific model and value as\n",
    "                      best clusters HDBSCAN object\n",
    "\n",
    "    Returns:\n",
    "        df_combined: dataframe of all documents with labels from\n",
    "                     best clusters for each model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    df_combined = df_ground.copy()\n",
    "    \n",
    "    for key, value in cluster_dict.items():\n",
    "        df_combined[key] = value.labels_\n",
    "    \n",
    "    return df_combined\n",
    "def summarize_results(results_dict, results_df):\n",
    "    \"\"\"\n",
    "    Returns a table summarizing each model's performance compared to ground\n",
    "    truth labels and the model's hyperparametes\n",
    "\n",
    "    Arguments:\n",
    "        results_dict: dict, key is the model name and value is a list of: \n",
    "                      model column name in combine_results output, best_params and best_clusters \n",
    "                      for each model (e.g. ['label_use', best_params_use, trials_use])\n",
    "        results_df: dataframe output of combine_results function; dataframe of all documents \n",
    "                    with labels from best clusters for each model\n",
    "\n",
    "    Returns:\n",
    "        df_final: dataframe with each row including a model name, calculated ARI and NMI,\n",
    "                  loss, label count, and hyperparameters of best model\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    summary = []\n",
    "\n",
    "    for key, value in results_dict.items():\n",
    "        ground_label = results_df['clustered_id'].values\n",
    "        predicted_label = results_df[value[0]].values\n",
    "        \n",
    "        ari = np.round(adjusted_rand_score(ground_label, predicted_label), 3)\n",
    "        nmi = np.round(normalized_mutual_info_score(ground_label, predicted_label), 3)\n",
    "        loss = value[2].best_trial['result']['loss']\n",
    "        label_count = value[2].best_trial['result']['label_count']\n",
    "        n_neighbors = value[1]['n_neighbors']\n",
    "        n_components = value[1]['n_components']\n",
    "        min_cluster_size = value[1]['min_cluster_size']\n",
    "        random_state = value[1]['random_state']\n",
    "        \n",
    "        summary.append([key, ari, nmi, loss, label_count, n_neighbors, n_components, \n",
    "                        min_cluster_size, random_state])\n",
    "\n",
    "    df_final = pd.DataFrame(summary, columns=['Model', 'ARI', 'NMI', 'loss', \n",
    "                                              'label_count', 'n_neighbors',\n",
    "                                              'n_components', 'min_cluster_size',\n",
    "                                              'random_state'])\n",
    "    \n",
    "    return df_final.sort_values(by='NMI', ascending=False)\n",
    "    \n",
    "def plot_clusters(embeddings, clusters, n_neighbors=15, min_dist=0.1):\n",
    "    \"\"\"\n",
    "    Reduce dimensionality of best clusters and plot in 2D\n",
    "\n",
    "    Arguments:\n",
    "        embeddings: embeddings to use\n",
    "        clusteres: HDBSCAN object of clusters\n",
    "        n_neighbors: float, UMAP hyperparameter n_neighbors\n",
    "        min_dist: float, UMAP hyperparameter min_dist for effective\n",
    "                  minimum distance between embedded points\n",
    "\n",
    "    \"\"\"\n",
    "    umap_data = umap.UMAP(n_neighbors=n_neighbors, \n",
    "                          n_components=2, \n",
    "                          min_dist = min_dist,  \n",
    "                          #metric='cosine',\n",
    "                          random_state=42).fit_transform(embeddings)\n",
    "\n",
    "    point_size = 100.0 / np.sqrt(embeddings.shape[0])\n",
    "    \n",
    "    result = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
    "    result['labels'] = clusters.labels_\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    outliers = result[result.labels == -1]\n",
    "    clustered = result[result.labels != -1]\n",
    "    plt.scatter(outliers.x, outliers.y, color = 'lightgrey', s=point_size)\n",
    "    plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=point_size, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_default = generate_clusters(embeddings_st1, \n",
    "                                     n_neighbors = 15, \n",
    "                                     n_components = 5, \n",
    "                                     min_cluster_size = 10,\n",
    "                                     random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594\n",
      "0.3238128447346732\n"
     ]
    }
   ],
   "source": [
    "labels_def, cost_def = score_clusters(clusters_default)\n",
    "print(labels_def)\n",
    "print(cost_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hspace = {\n",
    "    \"n_neighbors\": hp.choice('n_neighbors', range(3,16)),\n",
    "    \"n_components\": hp.choice('n_components', range(10,20)),\n",
    "    \"min_cluster_size\": hp.choice('min_cluster_size', range(16,36)),\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "label_lower = 1500\n",
    "label_upper = 4000\n",
    "max_evals = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [59:03<00:00, 35.43s/trial, best loss: 0.3293888290116021]  \n",
      "best:\n",
      "{'min_cluster_size': 16, 'n_components': 16, 'n_neighbors': 3, 'random_state': 42}\n",
      "label count: 581\n"
     ]
    }
   ],
   "source": [
    "best_params_use, best_clusters_use, trials_use = bayesian_search(embeddings_use, \n",
    "                                                                 space=hspace, \n",
    "                                                                 label_lower=label_lower, \n",
    "                                                                 label_upper=label_upper, \n",
    "                                                                 max_evals=max_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [56:36<00:00, 33.96s/trial, best loss: 0.36527293476193495]\n",
      "best:\n",
      "{'min_cluster_size': 20, 'n_components': 15, 'n_neighbors': 3, 'random_state': 42}\n",
      "label count: 426\n"
     ]
    }
   ],
   "source": [
    "best_params_st1, best_clusters_st1, trials_st1 = bayesian_search(embeddings_st1, \n",
    "                                                                 space=hspace, \n",
    "                                                                 label_lower=label_lower, \n",
    "                                                                 label_upper=label_upper, \n",
    "                                                                 max_evals=max_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [55:43<00:00, 33.43s/trial, best loss: 0.34444620554111455]\n",
      "best:\n",
      "{'min_cluster_size': 20, 'n_components': 12, 'n_neighbors': 3, 'random_state': 42}\n",
      "label count: 468\n"
     ]
    }
   ],
   "source": [
    "best_params_st2, best_clusters_st2, trials_st2 = bayesian_search(embeddings_st2, \n",
    "                                                                 space=hspace, \n",
    "                                                                 label_lower=label_lower, \n",
    "                                                                 label_upper=label_upper, \n",
    "                                                                 max_evals=max_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [55:38<00:00, 33.38s/trial, best loss: 0.3729759715970329] \n",
      "best:\n",
      "{'min_cluster_size': 18, 'n_components': 16, 'n_neighbors': 3, 'random_state': 42}\n",
      "label count: 464\n"
     ]
    }
   ],
   "source": [
    "best_params_st3, best_clusters_st3, trials_st3 = bayesian_search(embeddings_st3, \n",
    "                                                                 space=hspace, \n",
    "                                                                 label_lower=label_lower, \n",
    "                                                                 label_upper=label_upper, \n",
    "                                                                 max_evals=max_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_clusters_use' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\selsabrouty\\OneDrive - Deloitte (O365D)\\Documents\\Project Tilt\\new-tilt-nlp-preprocessing\\src\\scripts\\5v3. HDBSCAN+UMAP Clustering.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/selsabrouty/OneDrive%20-%20Deloitte%20%28O365D%29/Documents/Project%20Tilt/new-tilt-nlp-preprocessing/src/scripts/5v3.%20HDBSCAN%2BUMAP%20Clustering.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cluster_dict \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mlabel_use\u001b[39m\u001b[39m'\u001b[39m: best_clusters_use,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/selsabrouty/OneDrive%20-%20Deloitte%20%28O365D%29/Documents/Project%20Tilt/new-tilt-nlp-preprocessing/src/scripts/5v3.%20HDBSCAN%2BUMAP%20Clustering.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mlabel_st1\u001b[39m\u001b[39m'\u001b[39m: best_clusters_st1, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/selsabrouty/OneDrive%20-%20Deloitte%20%28O365D%29/Documents/Project%20Tilt/new-tilt-nlp-preprocessing/src/scripts/5v3.%20HDBSCAN%2BUMAP%20Clustering.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mlabel_st2\u001b[39m\u001b[39m'\u001b[39m: best_clusters_st2,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/selsabrouty/OneDrive%20-%20Deloitte%20%28O365D%29/Documents/Project%20Tilt/new-tilt-nlp-preprocessing/src/scripts/5v3.%20HDBSCAN%2BUMAP%20Clustering.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mlabel_st3\u001b[39m\u001b[39m'\u001b[39m: best_clusters_st3}\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/selsabrouty/OneDrive%20-%20Deloitte%20%28O365D%29/Documents/Project%20Tilt/new-tilt-nlp-preprocessing/src/scripts/5v3.%20HDBSCAN%2BUMAP%20Clustering.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m results_df \u001b[39m=\u001b[39m combine_results(full_df[[\u001b[39m'\u001b[39m\u001b[39mcleaned_text\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mclustered_id\u001b[39m\u001b[39m'\u001b[39m]], cluster_dict)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/selsabrouty/OneDrive%20-%20Deloitte%20%28O365D%29/Documents/Project%20Tilt/new-tilt-nlp-preprocessing/src/scripts/5v3.%20HDBSCAN%2BUMAP%20Clustering.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model_dict \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mUSE\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39mlabel_use\u001b[39m\u001b[39m'\u001b[39m, best_params_use, trials_use],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/selsabrouty/OneDrive%20-%20Deloitte%20%28O365D%29/Documents/Project%20Tilt/new-tilt-nlp-preprocessing/src/scripts/5v3.%20HDBSCAN%2BUMAP%20Clustering.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m               \u001b[39m'\u001b[39m\u001b[39mall-mpnet-base-v2\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39mlabel_st1\u001b[39m\u001b[39m'\u001b[39m, best_params_st1, trials_st1],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/selsabrouty/OneDrive%20-%20Deloitte%20%28O365D%29/Documents/Project%20Tilt/new-tilt-nlp-preprocessing/src/scripts/5v3.%20HDBSCAN%2BUMAP%20Clustering.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m               \u001b[39m'\u001b[39m\u001b[39mall-MiniLM-L6-v2\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39mlabel_st2\u001b[39m\u001b[39m'\u001b[39m, best_params_st2, trials_st2],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/selsabrouty/OneDrive%20-%20Deloitte%20%28O365D%29/Documents/Project%20Tilt/new-tilt-nlp-preprocessing/src/scripts/5v3.%20HDBSCAN%2BUMAP%20Clustering.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m               \u001b[39m'\u001b[39m\u001b[39mall-distilroberta-v1\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39mlabel_st3\u001b[39m\u001b[39m'\u001b[39m, best_params_st3, trials_st3]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/selsabrouty/OneDrive%20-%20Deloitte%20%28O365D%29/Documents/Project%20Tilt/new-tilt-nlp-preprocessing/src/scripts/5v3.%20HDBSCAN%2BUMAP%20Clustering.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_clusters_use' is not defined"
     ]
    }
   ],
   "source": [
    "cluster_dict = {'label_use': best_clusters_use,\n",
    "                'label_st1': best_clusters_st1, \n",
    "                'label_st2': best_clusters_st2,\n",
    "                'label_st3': best_clusters_st3}\n",
    "\n",
    "results_df = combine_results(full_df[['cleaned_text', 'clustered_id']], cluster_dict)\n",
    "    \n",
    "model_dict = {'USE': ['label_use', best_params_use, trials_use],\n",
    "              'all-mpnet-base-v2': ['label_st1', best_params_st1, trials_st1],\n",
    "              'all-MiniLM-L6-v2': ['label_st2', best_params_st2, trials_st2],\n",
    "              'all-distilroberta-v1': ['label_st3', best_params_st3, trials_st3]\n",
    "               }\n",
    "\n",
    "summarize_results(model_dict, results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
